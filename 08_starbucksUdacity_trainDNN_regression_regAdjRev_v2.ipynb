{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_starbucksUdacity_trainDNN_regression_regAdjRev_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkeeTFiGNg_h"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "% matplotlib inline\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "filePath = \"/content/drive/My Drive/Udacity-MLE-Capstone-Starbucks-data/\"\n",
        "\n",
        "# read in the json files\n",
        "#portfolio = pd.read_json(filePath+'data/portfolio.json', orient='records', lines=True)\n",
        "#profile = pd.read_json(filePath+'data/profile.json', orient='records', lines=True)\n",
        "#transcript = pd.read_json(filePath+'data/transcript.json', orient='records', lines=True)\n",
        "\n",
        "\n",
        "# Load data for training\n",
        "X = pd.read_pickle(filePath+'dataOfferCompAdjRevX.pkl')\n",
        "Y = pd.read_pickle(filePath+'dataOfferCompAdjRevY.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrYmJpfqx9fz",
        "outputId": "456f81f4-d365-4123-a779-3703d4a8cda1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# setting up torch device:\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('GPU is available.' if torch.cuda.is_available() else 'GPU is NOT avaliable, using CPU.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-7C4x680SRl"
      },
      "source": [
        "# Prepare and Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df23K5v-CHrQ",
        "outputId": "153ffe36-9795-44a6-b623-0057ebed2192"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1        21.05\n",
              "6         7.11\n",
              "7        20.20\n",
              "9        23.82\n",
              "10       17.63\n",
              "         ...  \n",
              "66487    20.34\n",
              "66490    10.59\n",
              "66491    11.76\n",
              "66499    13.68\n",
              "66500    13.42\n",
              "Name: adjRev, Length: 20459, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "aS54gI8aBu5e",
        "outputId": "dd170168-1cd4-49bc-f5d2-c1f077eabe79"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>difficulty</th>\n",
              "      <th>duration</th>\n",
              "      <th>reward</th>\n",
              "      <th>chan_social</th>\n",
              "      <th>chan_email</th>\n",
              "      <th>chan_web</th>\n",
              "      <th>chan_mobile</th>\n",
              "      <th>offer_type_bogo</th>\n",
              "      <th>offer_type_discount</th>\n",
              "      <th>offer_type_informational</th>\n",
              "      <th>age</th>\n",
              "      <th>income</th>\n",
              "      <th>joinDate_month</th>\n",
              "      <th>joinDate_year</th>\n",
              "      <th>gender_F</th>\n",
              "      <th>gender_M</th>\n",
              "      <th>gender_O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.048193</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.048193</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.048193</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.048193</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66487</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.433735</td>\n",
              "      <td>0.877778</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66490</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.819277</td>\n",
              "      <td>0.422222</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66491</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.819277</td>\n",
              "      <td>0.422222</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66499</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.156627</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66500</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.493976</td>\n",
              "      <td>0.155556</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20459 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       difficulty  duration  reward  ...  gender_F  gender_M  gender_O\n",
              "1            0.50  0.571429     1.0  ...         0         1         0\n",
              "6            0.50  0.285714     1.0  ...         0         1         0\n",
              "7            0.50  0.285714     1.0  ...         0         1         0\n",
              "9            1.00  1.000000     0.5  ...         0         1         0\n",
              "10           0.35  0.571429     0.3  ...         0         1         0\n",
              "...           ...       ...     ...  ...       ...       ...       ...\n",
              "66487        0.50  0.571429     0.2  ...         1         0         0\n",
              "66490        0.50  0.571429     0.2  ...         1         0         0\n",
              "66491        0.50  0.571429     0.2  ...         1         0         0\n",
              "66499        0.50  0.571429     0.2  ...         0         1         0\n",
              "66500        0.50  0.571429     0.2  ...         0         1         0\n",
              "\n",
              "[20459 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thNmgrvdZMPx",
        "outputId": "d23ab7b3-c7eb-4894-82cc-13d342aae63e"
      },
      "source": [
        "# Print columns used as model features\n",
        "print(X.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['difficulty', 'duration', 'reward', 'chan_social', 'chan_email',\n",
            "       'chan_web', 'chan_mobile', 'offer_type_bogo', 'offer_type_discount',\n",
            "       'offer_type_informational', 'age', 'income', 'joinDate_month',\n",
            "       'joinDate_year', 'gender_F', 'gender_M', 'gender_O'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4BR1fHd0L4V"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import max_error\n",
        "from sklearn.metrics import explained_variance_score\n",
        "\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X = min_max_scaler.fit_transform(X)\n",
        "\n",
        "# We split the dataset into 2/3 training and 1/3 testing sets.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n",
        "\n",
        "# Then we split the training set further into 2/3 training and 1/3 validation sets.\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z-B_fcoBFaa",
        "outputId": "cd193a30-b039-4ab5-9a96-67cf8e9c9f4e"
      },
      "source": [
        "print(X_train)\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         1.         0.375      ... 1.         0.         0.        ]\n",
            " [0.13333333 0.4        0.125      ... 1.         0.         0.        ]\n",
            " [0.33333333 1.         0.         ... 1.         0.         0.        ]\n",
            " ...\n",
            " [0.33333333 1.         0.         ... 0.         1.         0.        ]\n",
            " [0.         0.         0.375      ... 1.         0.         0.        ]\n",
            " [0.33333333 0.4        1.         ... 0.         1.         0.        ]]\n",
            "(13093, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg3kmuf5BLXC",
        "outputId": "7bf53cb9-4948-446a-92c9-e64e1f2790d2"
      },
      "source": [
        "print(Y_train)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52653    19.36\n",
            "10096     7.32\n",
            "25861    11.92\n",
            "22142     2.70\n",
            "17074     1.05\n",
            "         ...  \n",
            "20145     1.16\n",
            "59836     3.34\n",
            "64261    10.74\n",
            "13472    11.71\n",
            "9534      0.61\n",
            "Name: adjRev, Length: 13093, dtype: float64\n",
            "(13093,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INvHMQzH0MU_",
        "outputId": "26ea4e6f-6c09-4da8-e2f3-7117d65f9b78"
      },
      "source": [
        "#test_tensor = torch.Tensor(X.values)\n",
        "\n",
        "# Convert data into torch.Tensor:\n",
        "X_train = torch.Tensor(X_train).to(device)\n",
        "X_val = torch.Tensor(X_val).to(device)\n",
        "X_test = torch.Tensor(X_test).to(device)\n",
        "\n",
        "Y_train = torch.Tensor(Y_train.values).to(device)\n",
        "Y_val = torch.Tensor(Y_val.values).to(device)\n",
        "Y_test = torch.Tensor(Y_test.values).to(device)\n",
        "\n",
        "# Create datasets for the dataloaders:\n",
        "train_data = TensorDataset(X_train, Y_train)\n",
        "test_data = TensorDataset(X_test, Y_test)\n",
        "val_data = TensorDataset(X_val, Y_val)\n",
        "\n",
        "# print out some data stats\n",
        "print('# of training samples: ', len(train_data))\n",
        "print('# of validation samples: ', len(val_data))\n",
        "print('# of test samples: ', len(test_data))\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Creating the data loaders:\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "loaders = {\n",
        "    'train': train_loader,\n",
        "    'valid': val_loader,\n",
        "    'test': test_loader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of training samples:  13093\n",
            "# of validation samples:  3274\n",
            "# of test samples:  4092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F57evGG8FW9",
        "outputId": "ff3bf578-e830-432d-b102-2bcc0d90f0d4"
      },
      "source": [
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([13093, 17]) torch.Size([13093])\n",
            "torch.Size([4092, 17]) torch.Size([4092])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWHm4f3w0axJ",
        "outputId": "ab3e3d54-f7b7-4499-b3a0-14527ccb7772"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "n_classes = 1 # Number of classes\n",
        "n_features = 17 # Number of features\n",
        "\n",
        "# define the DNN architecture\n",
        "class Net(nn.Module):\n",
        "    ### TODO: choose an architecture, and complete the class\n",
        "    def __init__(self, n_features):\n",
        "        super(Net, self).__init__()\n",
        "        ## Define layers of a DNN\n",
        "        \n",
        "        self.fc1 = nn.Linear(n_features, 32)\n",
        "        # linear layer (17 -> 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc3 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc4 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc5 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc6 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc7 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc8 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc9 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc10 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc11 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc12 = nn.Linear(32, 32)\n",
        "        # linear layer (32 -> 32)\n",
        "        self.fc13 = nn.Linear(32, 32)        \n",
        "        # linear layer (32 -> 16)\n",
        "        self.fc14 = nn.Linear(32, 16)\n",
        "        # linear layer (16 -> 8)\n",
        "        self.fc15 = nn.Linear(16, 8)\n",
        "        # linear layer (8 -> 1)\n",
        "        self.fc16 = nn.Linear(8, n_classes)\n",
        "        \n",
        "        # dropout layer (p=0.25)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ## Define forward behavior\n",
        "        \n",
        "        # add 1st hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        \n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc3(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc4(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc5(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc6(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc7(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc8(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc9(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc10(x))\n",
        "\n",
        "        #print(\"a4: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 3rd hidden layer\n",
        "        x = F.leaky_relu(self.fc11(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc12(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc13(x))\n",
        "\n",
        "        #print(\"a3: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = F.leaky_relu(self.fc14(x))\n",
        "\n",
        "        #print(\"a4: \",x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 3rd hidden layer\n",
        "        x = F.leaky_relu(self.fc15(x))\n",
        "\n",
        "        #print(\"a4: \",x.shape)\n",
        "        # add dropout layer\n",
        "        #x = self.dropout(x)\n",
        "        # add 3rd hidden layer\n",
        "        x = self.fc16(x)\n",
        "\n",
        "        #print(\"a5: \",x.shape)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "# instantiate the CNN\n",
        "model = Net(n_features)\n",
        "print(model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=17, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc5): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc6): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc7): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc8): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc9): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc10): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc11): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc12): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc13): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc14): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (fc15): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (fc16): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7diViZVe5qaS"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.MSELoss()  # this is for regression mean squared loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfYsDq4f_56D"
      },
      "source": [
        "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    \n",
        "    print(\"Begin training...\")\n",
        "    \n",
        "    train_losses, valid_losses = [], []\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        #print(\"Epoch: \",epoch)\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        \n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "            #if batch_idx % 100 == 0:\n",
        "                #print(\"Epoch: {}, Batch: {}\".format(epoch,batch_idx))\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## find the loss and update the model parameters accordingly\n",
        "            ## record the average training loss, using something like\n",
        "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "            \n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            output = output.view(-1)\n",
        "            target = target.unsqueeze(0).view(-1)\n",
        "\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            \n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            \n",
        "            # record the average training loss, using something like\n",
        "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "            \n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        with torch.no_grad():\n",
        "          model.eval()\n",
        "          for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "              # move to GPU\n",
        "              if use_cuda:\n",
        "                  data, target = data.cuda(), target.cuda()\n",
        "              ## update the average validation loss\n",
        "\n",
        "              # forward pass: compute predicted outputs by passing inputs to the model\n",
        "              output = model(data)\n",
        "              #output = outputs.view(1, -1)  # make it the same shape as output\n",
        "              target = target.unsqueeze(0).view(-1)\n",
        "              output = output.view(-1)\n",
        "\n",
        "              # calculate the batch loss\n",
        "              loss = criterion(output, target)\n",
        "              \n",
        "              # update average validation loss \n",
        "              valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "        \n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if valid_loss < valid_loss_min:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min, valid_loss))\n",
        "                \n",
        "            valid_loss_min = valid_loss\n",
        "    \n",
        "    print(\"Training Complete!\")\n",
        "    # return trained model\n",
        "    return model, train_losses, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5VI8jGm7xJo",
        "outputId": "4ea7ea01-0611-48b4-8a7f-2e959280b94b"
      },
      "source": [
        "n_epochs = 500 # Number of training epochs\n",
        "\n",
        "# train the model\n",
        "model, train_losses, valid_losses = train(n_epochs, loaders, model, optimizer, \n",
        "                                          criterion, use_cuda, 'model_scratchReg.pt')\n",
        "\n",
        "# load the model that got the best validation accuracy\n",
        "model.load_state_dict(torch.load('model_scratchReg.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin training...\n",
            "Epoch: 1 \tTraining Loss: 105.450073 \tValidation Loss: 53.319355\n",
            "Validation loss decreased (inf --> 53.319355).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 57.115452 \tValidation Loss: 39.073677\n",
            "Validation loss decreased (53.319355 --> 39.073677).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 48.447628 \tValidation Loss: 42.168484\n",
            "Epoch: 4 \tTraining Loss: 46.133266 \tValidation Loss: 37.773170\n",
            "Validation loss decreased (39.073677 --> 37.773170).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 43.247433 \tValidation Loss: 36.063271\n",
            "Validation loss decreased (37.773170 --> 36.063271).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 43.009441 \tValidation Loss: 45.586262\n",
            "Epoch: 7 \tTraining Loss: 40.949028 \tValidation Loss: 40.125378\n",
            "Epoch: 8 \tTraining Loss: 40.519485 \tValidation Loss: 45.624901\n",
            "Epoch: 9 \tTraining Loss: 39.823357 \tValidation Loss: 37.788067\n",
            "Epoch: 10 \tTraining Loss: 38.712486 \tValidation Loss: 36.765839\n",
            "Epoch: 11 \tTraining Loss: 38.338764 \tValidation Loss: 39.568821\n",
            "Epoch: 12 \tTraining Loss: 38.269764 \tValidation Loss: 38.952137\n",
            "Epoch: 13 \tTraining Loss: 37.891464 \tValidation Loss: 38.575794\n",
            "Epoch: 14 \tTraining Loss: 37.971512 \tValidation Loss: 40.162689\n",
            "Epoch: 15 \tTraining Loss: 37.167755 \tValidation Loss: 40.816734\n",
            "Epoch: 16 \tTraining Loss: 37.305649 \tValidation Loss: 37.564808\n",
            "Epoch: 17 \tTraining Loss: 37.198578 \tValidation Loss: 41.395863\n",
            "Epoch: 18 \tTraining Loss: 37.030724 \tValidation Loss: 39.230991\n",
            "Epoch: 19 \tTraining Loss: 36.299782 \tValidation Loss: 38.418236\n",
            "Epoch: 20 \tTraining Loss: 36.693508 \tValidation Loss: 36.682472\n",
            "Epoch: 21 \tTraining Loss: 36.203510 \tValidation Loss: 38.179707\n",
            "Epoch: 22 \tTraining Loss: 35.960358 \tValidation Loss: 36.999462\n",
            "Epoch: 23 \tTraining Loss: 35.925201 \tValidation Loss: 38.207886\n",
            "Epoch: 24 \tTraining Loss: 35.959270 \tValidation Loss: 36.693703\n",
            "Epoch: 25 \tTraining Loss: 36.245468 \tValidation Loss: 35.591648\n",
            "Validation loss decreased (36.063271 --> 35.591648).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 35.911827 \tValidation Loss: 41.403629\n",
            "Epoch: 27 \tTraining Loss: 35.650742 \tValidation Loss: 36.989807\n",
            "Epoch: 28 \tTraining Loss: 35.805626 \tValidation Loss: 38.048923\n",
            "Epoch: 29 \tTraining Loss: 35.418247 \tValidation Loss: 36.665447\n",
            "Epoch: 30 \tTraining Loss: 35.768711 \tValidation Loss: 36.513931\n",
            "Epoch: 31 \tTraining Loss: 35.487965 \tValidation Loss: 37.881733\n",
            "Epoch: 32 \tTraining Loss: 35.626820 \tValidation Loss: 35.343952\n",
            "Validation loss decreased (35.591648 --> 35.343952).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 35.589699 \tValidation Loss: 39.945248\n",
            "Epoch: 34 \tTraining Loss: 35.455097 \tValidation Loss: 37.600475\n",
            "Epoch: 35 \tTraining Loss: 35.245049 \tValidation Loss: 36.786259\n",
            "Epoch: 36 \tTraining Loss: 35.596462 \tValidation Loss: 35.057079\n",
            "Validation loss decreased (35.343952 --> 35.057079).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 35.365154 \tValidation Loss: 38.122684\n",
            "Epoch: 38 \tTraining Loss: 35.189247 \tValidation Loss: 36.429306\n",
            "Epoch: 39 \tTraining Loss: 35.561813 \tValidation Loss: 36.804142\n",
            "Epoch: 40 \tTraining Loss: 35.089825 \tValidation Loss: 36.181396\n",
            "Epoch: 41 \tTraining Loss: 35.064007 \tValidation Loss: 38.504086\n",
            "Epoch: 42 \tTraining Loss: 35.736439 \tValidation Loss: 37.997925\n",
            "Epoch: 43 \tTraining Loss: 34.970428 \tValidation Loss: 37.129608\n",
            "Epoch: 44 \tTraining Loss: 35.375603 \tValidation Loss: 36.517372\n",
            "Epoch: 45 \tTraining Loss: 35.288532 \tValidation Loss: 37.261093\n",
            "Epoch: 46 \tTraining Loss: 34.949238 \tValidation Loss: 36.225548\n",
            "Epoch: 47 \tTraining Loss: 35.148254 \tValidation Loss: 37.221107\n",
            "Epoch: 48 \tTraining Loss: 34.773705 \tValidation Loss: 36.267403\n",
            "Epoch: 49 \tTraining Loss: 34.978546 \tValidation Loss: 37.277348\n",
            "Epoch: 50 \tTraining Loss: 35.013466 \tValidation Loss: 38.253464\n",
            "Epoch: 51 \tTraining Loss: 34.868515 \tValidation Loss: 36.779415\n",
            "Epoch: 52 \tTraining Loss: 34.818604 \tValidation Loss: 38.420845\n",
            "Epoch: 53 \tTraining Loss: 34.733147 \tValidation Loss: 39.742744\n",
            "Epoch: 54 \tTraining Loss: 34.632481 \tValidation Loss: 38.210278\n",
            "Epoch: 55 \tTraining Loss: 34.445351 \tValidation Loss: 36.836552\n",
            "Epoch: 56 \tTraining Loss: 34.776836 \tValidation Loss: 35.068890\n",
            "Epoch: 57 \tTraining Loss: 34.734924 \tValidation Loss: 39.014111\n",
            "Epoch: 58 \tTraining Loss: 34.533611 \tValidation Loss: 37.671654\n",
            "Epoch: 59 \tTraining Loss: 34.667774 \tValidation Loss: 39.659111\n",
            "Epoch: 60 \tTraining Loss: 34.929199 \tValidation Loss: 37.681129\n",
            "Epoch: 61 \tTraining Loss: 34.549160 \tValidation Loss: 36.050030\n",
            "Epoch: 62 \tTraining Loss: 34.569443 \tValidation Loss: 35.944500\n",
            "Epoch: 63 \tTraining Loss: 34.763664 \tValidation Loss: 37.569309\n",
            "Epoch: 64 \tTraining Loss: 34.265503 \tValidation Loss: 37.570362\n",
            "Epoch: 65 \tTraining Loss: 34.641663 \tValidation Loss: 35.537022\n",
            "Epoch: 66 \tTraining Loss: 34.644070 \tValidation Loss: 37.725132\n",
            "Epoch: 67 \tTraining Loss: 34.979969 \tValidation Loss: 37.881332\n",
            "Epoch: 68 \tTraining Loss: 34.482311 \tValidation Loss: 36.829231\n",
            "Epoch: 69 \tTraining Loss: 34.832741 \tValidation Loss: 36.916458\n",
            "Epoch: 70 \tTraining Loss: 34.684845 \tValidation Loss: 36.674717\n",
            "Epoch: 71 \tTraining Loss: 34.140110 \tValidation Loss: 38.068169\n",
            "Epoch: 72 \tTraining Loss: 34.570763 \tValidation Loss: 36.707825\n",
            "Epoch: 73 \tTraining Loss: 34.260578 \tValidation Loss: 37.579620\n",
            "Epoch: 74 \tTraining Loss: 34.734997 \tValidation Loss: 36.899200\n",
            "Epoch: 75 \tTraining Loss: 34.556671 \tValidation Loss: 36.032322\n",
            "Epoch: 76 \tTraining Loss: 34.306087 \tValidation Loss: 36.189121\n",
            "Epoch: 77 \tTraining Loss: 34.294689 \tValidation Loss: 38.230122\n",
            "Epoch: 78 \tTraining Loss: 34.526089 \tValidation Loss: 36.131290\n",
            "Epoch: 79 \tTraining Loss: 35.040588 \tValidation Loss: 38.510677\n",
            "Epoch: 80 \tTraining Loss: 34.662460 \tValidation Loss: 36.679958\n",
            "Epoch: 81 \tTraining Loss: 34.420799 \tValidation Loss: 36.369587\n",
            "Epoch: 82 \tTraining Loss: 34.255657 \tValidation Loss: 36.649612\n",
            "Epoch: 83 \tTraining Loss: 34.279469 \tValidation Loss: 38.571926\n",
            "Epoch: 84 \tTraining Loss: 34.293400 \tValidation Loss: 38.141533\n",
            "Epoch: 85 \tTraining Loss: 34.277260 \tValidation Loss: 37.302250\n",
            "Epoch: 86 \tTraining Loss: 34.201794 \tValidation Loss: 36.629723\n",
            "Epoch: 87 \tTraining Loss: 34.441978 \tValidation Loss: 35.006390\n",
            "Validation loss decreased (35.057079 --> 35.006390).  Saving model ...\n",
            "Epoch: 88 \tTraining Loss: 34.551025 \tValidation Loss: 36.503700\n",
            "Epoch: 89 \tTraining Loss: 34.234833 \tValidation Loss: 36.716236\n",
            "Epoch: 90 \tTraining Loss: 34.189934 \tValidation Loss: 37.255260\n",
            "Epoch: 91 \tTraining Loss: 34.266472 \tValidation Loss: 38.431133\n",
            "Epoch: 92 \tTraining Loss: 34.248371 \tValidation Loss: 36.646248\n",
            "Epoch: 93 \tTraining Loss: 34.206181 \tValidation Loss: 35.978661\n",
            "Epoch: 94 \tTraining Loss: 33.886139 \tValidation Loss: 36.157352\n",
            "Epoch: 95 \tTraining Loss: 34.058533 \tValidation Loss: 36.258121\n",
            "Epoch: 96 \tTraining Loss: 34.254009 \tValidation Loss: 35.741405\n",
            "Epoch: 97 \tTraining Loss: 34.135113 \tValidation Loss: 36.748039\n",
            "Epoch: 98 \tTraining Loss: 34.304089 \tValidation Loss: 36.583664\n",
            "Epoch: 99 \tTraining Loss: 34.196274 \tValidation Loss: 38.192196\n",
            "Epoch: 100 \tTraining Loss: 34.093376 \tValidation Loss: 35.973812\n",
            "Epoch: 101 \tTraining Loss: 33.930626 \tValidation Loss: 36.821602\n",
            "Epoch: 102 \tTraining Loss: 34.040974 \tValidation Loss: 36.360031\n",
            "Epoch: 103 \tTraining Loss: 33.916878 \tValidation Loss: 36.695702\n",
            "Epoch: 104 \tTraining Loss: 34.378880 \tValidation Loss: 34.917442\n",
            "Validation loss decreased (35.006390 --> 34.917442).  Saving model ...\n",
            "Epoch: 105 \tTraining Loss: 33.897995 \tValidation Loss: 36.653706\n",
            "Epoch: 106 \tTraining Loss: 33.990318 \tValidation Loss: 35.738029\n",
            "Epoch: 107 \tTraining Loss: 33.756802 \tValidation Loss: 36.232323\n",
            "Epoch: 108 \tTraining Loss: 33.805260 \tValidation Loss: 35.095936\n",
            "Epoch: 109 \tTraining Loss: 33.847176 \tValidation Loss: 36.758785\n",
            "Epoch: 110 \tTraining Loss: 34.065414 \tValidation Loss: 36.285686\n",
            "Epoch: 111 \tTraining Loss: 35.019478 \tValidation Loss: 35.553337\n",
            "Epoch: 112 \tTraining Loss: 34.146465 \tValidation Loss: 36.607048\n",
            "Epoch: 113 \tTraining Loss: 34.021622 \tValidation Loss: 36.045837\n",
            "Epoch: 114 \tTraining Loss: 33.872585 \tValidation Loss: 35.161095\n",
            "Epoch: 115 \tTraining Loss: 34.200947 \tValidation Loss: 35.927341\n",
            "Epoch: 116 \tTraining Loss: 33.916119 \tValidation Loss: 36.795822\n",
            "Epoch: 117 \tTraining Loss: 33.713894 \tValidation Loss: 36.544994\n",
            "Epoch: 118 \tTraining Loss: 33.974171 \tValidation Loss: 35.258736\n",
            "Epoch: 119 \tTraining Loss: 33.996147 \tValidation Loss: 36.010410\n",
            "Epoch: 120 \tTraining Loss: 33.782154 \tValidation Loss: 36.179958\n",
            "Epoch: 121 \tTraining Loss: 33.790268 \tValidation Loss: 36.451817\n",
            "Epoch: 122 \tTraining Loss: 33.746597 \tValidation Loss: 35.696236\n",
            "Epoch: 123 \tTraining Loss: 33.555172 \tValidation Loss: 35.520153\n",
            "Epoch: 124 \tTraining Loss: 33.748093 \tValidation Loss: 35.957909\n",
            "Epoch: 125 \tTraining Loss: 33.959801 \tValidation Loss: 35.393852\n",
            "Epoch: 126 \tTraining Loss: 33.887287 \tValidation Loss: 36.363262\n",
            "Epoch: 127 \tTraining Loss: 34.031467 \tValidation Loss: 35.553608\n",
            "Epoch: 128 \tTraining Loss: 33.898315 \tValidation Loss: 35.381268\n",
            "Epoch: 129 \tTraining Loss: 33.814125 \tValidation Loss: 35.049950\n",
            "Epoch: 130 \tTraining Loss: 33.839523 \tValidation Loss: 37.582897\n",
            "Epoch: 131 \tTraining Loss: 33.645638 \tValidation Loss: 35.873646\n",
            "Epoch: 132 \tTraining Loss: 33.904224 \tValidation Loss: 35.441940\n",
            "Epoch: 133 \tTraining Loss: 33.793308 \tValidation Loss: 35.679245\n",
            "Epoch: 134 \tTraining Loss: 33.975365 \tValidation Loss: 35.882900\n",
            "Epoch: 135 \tTraining Loss: 33.969326 \tValidation Loss: 37.990662\n",
            "Epoch: 136 \tTraining Loss: 34.028717 \tValidation Loss: 37.367378\n",
            "Epoch: 137 \tTraining Loss: 33.796219 \tValidation Loss: 36.781059\n",
            "Epoch: 138 \tTraining Loss: 33.668880 \tValidation Loss: 35.840961\n",
            "Epoch: 139 \tTraining Loss: 33.894382 \tValidation Loss: 36.075909\n",
            "Epoch: 140 \tTraining Loss: 33.682510 \tValidation Loss: 35.046734\n",
            "Epoch: 141 \tTraining Loss: 34.008987 \tValidation Loss: 35.145927\n",
            "Epoch: 142 \tTraining Loss: 33.834648 \tValidation Loss: 36.252628\n",
            "Epoch: 143 \tTraining Loss: 33.835617 \tValidation Loss: 35.656860\n",
            "Epoch: 144 \tTraining Loss: 33.876522 \tValidation Loss: 36.401634\n",
            "Epoch: 145 \tTraining Loss: 33.887230 \tValidation Loss: 36.595253\n",
            "Epoch: 146 \tTraining Loss: 33.810673 \tValidation Loss: 36.291641\n",
            "Epoch: 147 \tTraining Loss: 33.715202 \tValidation Loss: 36.884968\n",
            "Epoch: 148 \tTraining Loss: 33.722382 \tValidation Loss: 34.905582\n",
            "Validation loss decreased (34.917442 --> 34.905582).  Saving model ...\n",
            "Epoch: 149 \tTraining Loss: 33.762844 \tValidation Loss: 34.893578\n",
            "Validation loss decreased (34.905582 --> 34.893578).  Saving model ...\n",
            "Epoch: 150 \tTraining Loss: 33.843376 \tValidation Loss: 36.088951\n",
            "Epoch: 151 \tTraining Loss: 33.539509 \tValidation Loss: 35.806858\n",
            "Epoch: 152 \tTraining Loss: 33.479305 \tValidation Loss: 36.575409\n",
            "Epoch: 153 \tTraining Loss: 33.810047 \tValidation Loss: 35.929016\n",
            "Epoch: 154 \tTraining Loss: 33.870464 \tValidation Loss: 36.604500\n",
            "Epoch: 155 \tTraining Loss: 33.765053 \tValidation Loss: 35.519489\n",
            "Epoch: 156 \tTraining Loss: 33.690323 \tValidation Loss: 36.105896\n",
            "Epoch: 157 \tTraining Loss: 33.711243 \tValidation Loss: 36.757866\n",
            "Epoch: 158 \tTraining Loss: 33.792747 \tValidation Loss: 36.819843\n",
            "Epoch: 159 \tTraining Loss: 33.649010 \tValidation Loss: 35.805412\n",
            "Epoch: 160 \tTraining Loss: 33.681374 \tValidation Loss: 36.076942\n",
            "Epoch: 161 \tTraining Loss: 33.928238 \tValidation Loss: 35.379517\n",
            "Epoch: 162 \tTraining Loss: 34.714066 \tValidation Loss: 36.042240\n",
            "Epoch: 163 \tTraining Loss: 33.875568 \tValidation Loss: 35.121288\n",
            "Epoch: 164 \tTraining Loss: 33.725677 \tValidation Loss: 35.659733\n",
            "Epoch: 165 \tTraining Loss: 33.791107 \tValidation Loss: 35.358204\n",
            "Epoch: 166 \tTraining Loss: 33.723495 \tValidation Loss: 35.766376\n",
            "Epoch: 167 \tTraining Loss: 33.872353 \tValidation Loss: 36.065197\n",
            "Epoch: 168 \tTraining Loss: 34.003937 \tValidation Loss: 35.236832\n",
            "Epoch: 169 \tTraining Loss: 33.674141 \tValidation Loss: 36.388409\n",
            "Epoch: 170 \tTraining Loss: 33.940605 \tValidation Loss: 35.869614\n",
            "Epoch: 171 \tTraining Loss: 33.636295 \tValidation Loss: 35.876759\n",
            "Epoch: 172 \tTraining Loss: 33.539040 \tValidation Loss: 36.363487\n",
            "Epoch: 173 \tTraining Loss: 33.830032 \tValidation Loss: 35.794563\n",
            "Epoch: 174 \tTraining Loss: 33.603722 \tValidation Loss: 35.508919\n",
            "Epoch: 175 \tTraining Loss: 33.583351 \tValidation Loss: 37.082203\n",
            "Epoch: 176 \tTraining Loss: 33.688831 \tValidation Loss: 35.557423\n",
            "Epoch: 177 \tTraining Loss: 33.571808 \tValidation Loss: 35.029285\n",
            "Epoch: 178 \tTraining Loss: 33.415951 \tValidation Loss: 35.300823\n",
            "Epoch: 179 \tTraining Loss: 33.601418 \tValidation Loss: 36.057007\n",
            "Epoch: 180 \tTraining Loss: 33.571278 \tValidation Loss: 36.249458\n",
            "Epoch: 181 \tTraining Loss: 33.710915 \tValidation Loss: 36.082153\n",
            "Epoch: 182 \tTraining Loss: 33.241558 \tValidation Loss: 36.033314\n",
            "Epoch: 183 \tTraining Loss: 33.700657 \tValidation Loss: 36.584770\n",
            "Epoch: 184 \tTraining Loss: 33.484932 \tValidation Loss: 35.305801\n",
            "Epoch: 185 \tTraining Loss: 33.451298 \tValidation Loss: 36.384449\n",
            "Epoch: 186 \tTraining Loss: 34.147030 \tValidation Loss: 35.525101\n",
            "Epoch: 187 \tTraining Loss: 33.564766 \tValidation Loss: 35.432243\n",
            "Epoch: 188 \tTraining Loss: 33.529346 \tValidation Loss: 35.199516\n",
            "Epoch: 189 \tTraining Loss: 33.414631 \tValidation Loss: 37.169682\n",
            "Epoch: 190 \tTraining Loss: 33.629124 \tValidation Loss: 35.128910\n",
            "Epoch: 191 \tTraining Loss: 33.417774 \tValidation Loss: 34.956715\n",
            "Epoch: 192 \tTraining Loss: 33.414345 \tValidation Loss: 36.565861\n",
            "Epoch: 193 \tTraining Loss: 33.772938 \tValidation Loss: 34.810001\n",
            "Validation loss decreased (34.893578 --> 34.810001).  Saving model ...\n",
            "Epoch: 194 \tTraining Loss: 33.601414 \tValidation Loss: 35.889782\n",
            "Epoch: 195 \tTraining Loss: 33.415634 \tValidation Loss: 34.963882\n",
            "Epoch: 196 \tTraining Loss: 33.809505 \tValidation Loss: 36.946266\n",
            "Epoch: 197 \tTraining Loss: 33.571617 \tValidation Loss: 35.929394\n",
            "Epoch: 198 \tTraining Loss: 33.588585 \tValidation Loss: 35.317402\n",
            "Epoch: 199 \tTraining Loss: 33.817673 \tValidation Loss: 35.176216\n",
            "Epoch: 200 \tTraining Loss: 33.793701 \tValidation Loss: 34.317753\n",
            "Validation loss decreased (34.810001 --> 34.317753).  Saving model ...\n",
            "Epoch: 201 \tTraining Loss: 34.213615 \tValidation Loss: 35.010651\n",
            "Epoch: 202 \tTraining Loss: 33.750904 \tValidation Loss: 36.004105\n",
            "Epoch: 203 \tTraining Loss: 33.913403 \tValidation Loss: 36.683361\n",
            "Epoch: 204 \tTraining Loss: 33.645908 \tValidation Loss: 35.230366\n",
            "Epoch: 205 \tTraining Loss: 33.557289 \tValidation Loss: 35.858513\n",
            "Epoch: 206 \tTraining Loss: 33.591518 \tValidation Loss: 36.372238\n",
            "Epoch: 207 \tTraining Loss: 33.597210 \tValidation Loss: 34.797909\n",
            "Epoch: 208 \tTraining Loss: 33.257755 \tValidation Loss: 35.283039\n",
            "Epoch: 209 \tTraining Loss: 33.616447 \tValidation Loss: 35.151772\n",
            "Epoch: 210 \tTraining Loss: 33.374699 \tValidation Loss: 35.012409\n",
            "Epoch: 211 \tTraining Loss: 33.927090 \tValidation Loss: 36.950386\n",
            "Epoch: 212 \tTraining Loss: 33.956257 \tValidation Loss: 36.510784\n",
            "Epoch: 213 \tTraining Loss: 33.740005 \tValidation Loss: 35.262878\n",
            "Epoch: 214 \tTraining Loss: 33.406471 \tValidation Loss: 35.539833\n",
            "Epoch: 215 \tTraining Loss: 33.407764 \tValidation Loss: 34.917084\n",
            "Epoch: 216 \tTraining Loss: 33.339596 \tValidation Loss: 35.208344\n",
            "Epoch: 217 \tTraining Loss: 33.803253 \tValidation Loss: 35.941784\n",
            "Epoch: 218 \tTraining Loss: 33.760174 \tValidation Loss: 36.014606\n",
            "Epoch: 219 \tTraining Loss: 33.726711 \tValidation Loss: 35.875072\n",
            "Epoch: 220 \tTraining Loss: 33.805008 \tValidation Loss: 35.570835\n",
            "Epoch: 221 \tTraining Loss: 33.724747 \tValidation Loss: 36.564011\n",
            "Epoch: 222 \tTraining Loss: 33.729717 \tValidation Loss: 35.079517\n",
            "Epoch: 223 \tTraining Loss: 33.668308 \tValidation Loss: 36.083473\n",
            "Epoch: 224 \tTraining Loss: 33.476654 \tValidation Loss: 36.310986\n",
            "Epoch: 225 \tTraining Loss: 33.484859 \tValidation Loss: 34.659290\n",
            "Epoch: 226 \tTraining Loss: 33.614449 \tValidation Loss: 35.076607\n",
            "Epoch: 227 \tTraining Loss: 33.495770 \tValidation Loss: 34.977116\n",
            "Epoch: 228 \tTraining Loss: 33.598286 \tValidation Loss: 36.371166\n",
            "Epoch: 229 \tTraining Loss: 33.624813 \tValidation Loss: 35.271019\n",
            "Epoch: 230 \tTraining Loss: 33.507442 \tValidation Loss: 34.896008\n",
            "Epoch: 231 \tTraining Loss: 33.654934 \tValidation Loss: 34.951267\n",
            "Epoch: 232 \tTraining Loss: 33.510376 \tValidation Loss: 35.190083\n",
            "Epoch: 233 \tTraining Loss: 33.414139 \tValidation Loss: 35.860558\n",
            "Epoch: 234 \tTraining Loss: 33.507320 \tValidation Loss: 34.573524\n",
            "Epoch: 235 \tTraining Loss: 33.574787 \tValidation Loss: 36.013069\n",
            "Epoch: 236 \tTraining Loss: 33.557327 \tValidation Loss: 35.611797\n",
            "Epoch: 237 \tTraining Loss: 33.548298 \tValidation Loss: 36.144691\n",
            "Epoch: 238 \tTraining Loss: 33.803013 \tValidation Loss: 36.018463\n",
            "Epoch: 239 \tTraining Loss: 33.413258 \tValidation Loss: 35.142586\n",
            "Epoch: 240 \tTraining Loss: 33.349731 \tValidation Loss: 34.906799\n",
            "Epoch: 241 \tTraining Loss: 33.449268 \tValidation Loss: 35.444000\n",
            "Epoch: 242 \tTraining Loss: 33.457088 \tValidation Loss: 35.663010\n",
            "Epoch: 243 \tTraining Loss: 33.427860 \tValidation Loss: 34.855217\n",
            "Epoch: 244 \tTraining Loss: 33.445457 \tValidation Loss: 35.090466\n",
            "Epoch: 245 \tTraining Loss: 33.376030 \tValidation Loss: 35.861496\n",
            "Epoch: 246 \tTraining Loss: 33.239521 \tValidation Loss: 35.492111\n",
            "Epoch: 247 \tTraining Loss: 33.381332 \tValidation Loss: 35.360405\n",
            "Epoch: 248 \tTraining Loss: 33.400333 \tValidation Loss: 35.229061\n",
            "Epoch: 249 \tTraining Loss: 33.211552 \tValidation Loss: 35.198601\n",
            "Epoch: 250 \tTraining Loss: 33.430634 \tValidation Loss: 35.614223\n",
            "Epoch: 251 \tTraining Loss: 33.666275 \tValidation Loss: 35.287899\n",
            "Epoch: 252 \tTraining Loss: 33.361729 \tValidation Loss: 35.129627\n",
            "Epoch: 253 \tTraining Loss: 33.351326 \tValidation Loss: 34.647701\n",
            "Epoch: 254 \tTraining Loss: 33.438244 \tValidation Loss: 34.533623\n",
            "Epoch: 255 \tTraining Loss: 33.139526 \tValidation Loss: 34.721737\n",
            "Epoch: 256 \tTraining Loss: 33.538116 \tValidation Loss: 34.761799\n",
            "Epoch: 257 \tTraining Loss: 33.409908 \tValidation Loss: 35.179852\n",
            "Epoch: 258 \tTraining Loss: 33.433327 \tValidation Loss: 34.848610\n",
            "Epoch: 259 \tTraining Loss: 33.473778 \tValidation Loss: 34.858696\n",
            "Epoch: 260 \tTraining Loss: 33.293900 \tValidation Loss: 35.560806\n",
            "Epoch: 261 \tTraining Loss: 33.311268 \tValidation Loss: 34.952171\n",
            "Epoch: 262 \tTraining Loss: 33.381477 \tValidation Loss: 34.401436\n",
            "Epoch: 263 \tTraining Loss: 33.355396 \tValidation Loss: 34.954304\n",
            "Epoch: 264 \tTraining Loss: 33.570389 \tValidation Loss: 36.362209\n",
            "Epoch: 265 \tTraining Loss: 33.214787 \tValidation Loss: 35.375458\n",
            "Epoch: 266 \tTraining Loss: 33.246284 \tValidation Loss: 35.028992\n",
            "Epoch: 267 \tTraining Loss: 33.406109 \tValidation Loss: 35.373119\n",
            "Epoch: 268 \tTraining Loss: 33.502758 \tValidation Loss: 34.714153\n",
            "Epoch: 269 \tTraining Loss: 33.401180 \tValidation Loss: 34.473042\n",
            "Epoch: 270 \tTraining Loss: 33.709900 \tValidation Loss: 35.064232\n",
            "Epoch: 271 \tTraining Loss: 33.484234 \tValidation Loss: 35.926014\n",
            "Epoch: 272 \tTraining Loss: 33.412273 \tValidation Loss: 34.564980\n",
            "Epoch: 273 \tTraining Loss: 33.296448 \tValidation Loss: 34.695728\n",
            "Epoch: 274 \tTraining Loss: 34.076981 \tValidation Loss: 35.566082\n",
            "Epoch: 275 \tTraining Loss: 33.427937 \tValidation Loss: 35.700989\n",
            "Epoch: 276 \tTraining Loss: 33.519814 \tValidation Loss: 34.765144\n",
            "Epoch: 277 \tTraining Loss: 33.649353 \tValidation Loss: 34.991714\n",
            "Epoch: 278 \tTraining Loss: 33.707062 \tValidation Loss: 34.624729\n",
            "Epoch: 279 \tTraining Loss: 33.888012 \tValidation Loss: 35.320889\n",
            "Epoch: 280 \tTraining Loss: 33.618324 \tValidation Loss: 35.102791\n",
            "Epoch: 281 \tTraining Loss: 33.176655 \tValidation Loss: 34.502281\n",
            "Epoch: 282 \tTraining Loss: 33.494823 \tValidation Loss: 35.811970\n",
            "Epoch: 283 \tTraining Loss: 33.500778 \tValidation Loss: 35.504055\n",
            "Epoch: 284 \tTraining Loss: 33.459881 \tValidation Loss: 36.530807\n",
            "Epoch: 285 \tTraining Loss: 33.506073 \tValidation Loss: 34.851292\n",
            "Epoch: 286 \tTraining Loss: 33.540611 \tValidation Loss: 34.765011\n",
            "Epoch: 287 \tTraining Loss: 33.466503 \tValidation Loss: 35.086315\n",
            "Epoch: 288 \tTraining Loss: 33.332348 \tValidation Loss: 34.944927\n",
            "Epoch: 289 \tTraining Loss: 33.541168 \tValidation Loss: 35.297382\n",
            "Epoch: 290 \tTraining Loss: 33.448864 \tValidation Loss: 36.102116\n",
            "Epoch: 291 \tTraining Loss: 33.330887 \tValidation Loss: 35.695755\n",
            "Epoch: 292 \tTraining Loss: 33.404480 \tValidation Loss: 35.711006\n",
            "Epoch: 293 \tTraining Loss: 33.101154 \tValidation Loss: 35.588779\n",
            "Epoch: 294 \tTraining Loss: 33.890450 \tValidation Loss: 35.026150\n",
            "Epoch: 295 \tTraining Loss: 33.473476 \tValidation Loss: 35.261795\n",
            "Epoch: 296 \tTraining Loss: 33.377148 \tValidation Loss: 34.955860\n",
            "Epoch: 297 \tTraining Loss: 33.372982 \tValidation Loss: 35.303253\n",
            "Epoch: 298 \tTraining Loss: 33.488289 \tValidation Loss: 35.475811\n",
            "Epoch: 299 \tTraining Loss: 34.176468 \tValidation Loss: 35.105045\n",
            "Epoch: 300 \tTraining Loss: 33.702045 \tValidation Loss: 34.874695\n",
            "Epoch: 301 \tTraining Loss: 33.577518 \tValidation Loss: 35.265987\n",
            "Epoch: 302 \tTraining Loss: 33.266613 \tValidation Loss: 34.517105\n",
            "Epoch: 303 \tTraining Loss: 33.368137 \tValidation Loss: 34.751244\n",
            "Epoch: 304 \tTraining Loss: 33.525814 \tValidation Loss: 34.623920\n",
            "Epoch: 305 \tTraining Loss: 33.269497 \tValidation Loss: 35.438522\n",
            "Epoch: 306 \tTraining Loss: 33.451019 \tValidation Loss: 35.337578\n",
            "Epoch: 307 \tTraining Loss: 33.367451 \tValidation Loss: 35.347424\n",
            "Epoch: 308 \tTraining Loss: 33.382961 \tValidation Loss: 34.894047\n",
            "Epoch: 309 \tTraining Loss: 33.044151 \tValidation Loss: 35.143642\n",
            "Epoch: 310 \tTraining Loss: 33.490852 \tValidation Loss: 35.970894\n",
            "Epoch: 311 \tTraining Loss: 33.317039 \tValidation Loss: 34.938965\n",
            "Epoch: 312 \tTraining Loss: 33.329868 \tValidation Loss: 35.131950\n",
            "Epoch: 313 \tTraining Loss: 33.637966 \tValidation Loss: 35.127438\n",
            "Epoch: 314 \tTraining Loss: 33.370728 \tValidation Loss: 34.836922\n",
            "Epoch: 315 \tTraining Loss: 33.546494 \tValidation Loss: 35.174290\n",
            "Epoch: 316 \tTraining Loss: 33.300251 \tValidation Loss: 34.529682\n",
            "Epoch: 317 \tTraining Loss: 33.421501 \tValidation Loss: 35.611202\n",
            "Epoch: 318 \tTraining Loss: 33.250473 \tValidation Loss: 35.703957\n",
            "Epoch: 319 \tTraining Loss: 33.447662 \tValidation Loss: 34.843658\n",
            "Epoch: 320 \tTraining Loss: 33.364300 \tValidation Loss: 35.567905\n",
            "Epoch: 321 \tTraining Loss: 33.271481 \tValidation Loss: 35.516071\n",
            "Epoch: 322 \tTraining Loss: 33.426941 \tValidation Loss: 35.188942\n",
            "Epoch: 323 \tTraining Loss: 33.222599 \tValidation Loss: 34.400238\n",
            "Epoch: 324 \tTraining Loss: 33.508400 \tValidation Loss: 34.313354\n",
            "Validation loss decreased (34.317753 --> 34.313354).  Saving model ...\n",
            "Epoch: 325 \tTraining Loss: 33.460701 \tValidation Loss: 35.787395\n",
            "Epoch: 326 \tTraining Loss: 33.127789 \tValidation Loss: 34.961021\n",
            "Epoch: 327 \tTraining Loss: 33.151962 \tValidation Loss: 36.056999\n",
            "Epoch: 328 \tTraining Loss: 33.331146 \tValidation Loss: 35.060474\n",
            "Epoch: 329 \tTraining Loss: 33.220020 \tValidation Loss: 35.164223\n",
            "Epoch: 330 \tTraining Loss: 33.207394 \tValidation Loss: 35.610512\n",
            "Epoch: 331 \tTraining Loss: 33.359344 \tValidation Loss: 34.536880\n",
            "Epoch: 332 \tTraining Loss: 32.998310 \tValidation Loss: 35.416958\n",
            "Epoch: 333 \tTraining Loss: 33.227047 \tValidation Loss: 35.700954\n",
            "Epoch: 334 \tTraining Loss: 33.091007 \tValidation Loss: 35.450676\n",
            "Epoch: 335 \tTraining Loss: 33.158493 \tValidation Loss: 35.268101\n",
            "Epoch: 336 \tTraining Loss: 33.366467 \tValidation Loss: 34.937592\n",
            "Epoch: 337 \tTraining Loss: 33.154568 \tValidation Loss: 34.363876\n",
            "Epoch: 338 \tTraining Loss: 33.101734 \tValidation Loss: 34.877853\n",
            "Epoch: 339 \tTraining Loss: 33.506729 \tValidation Loss: 34.739380\n",
            "Epoch: 340 \tTraining Loss: 33.233501 \tValidation Loss: 36.284317\n",
            "Epoch: 341 \tTraining Loss: 33.402138 \tValidation Loss: 34.563587\n",
            "Epoch: 342 \tTraining Loss: 33.000263 \tValidation Loss: 35.810024\n",
            "Epoch: 343 \tTraining Loss: 33.586704 \tValidation Loss: 36.359146\n",
            "Epoch: 344 \tTraining Loss: 33.591320 \tValidation Loss: 35.254807\n",
            "Epoch: 345 \tTraining Loss: 33.297817 \tValidation Loss: 34.871410\n",
            "Epoch: 346 \tTraining Loss: 33.277805 \tValidation Loss: 35.437725\n",
            "Epoch: 347 \tTraining Loss: 33.157307 \tValidation Loss: 34.683392\n",
            "Epoch: 348 \tTraining Loss: 33.393734 \tValidation Loss: 35.128643\n",
            "Epoch: 349 \tTraining Loss: 33.233002 \tValidation Loss: 34.769466\n",
            "Epoch: 350 \tTraining Loss: 33.415691 \tValidation Loss: 34.507656\n",
            "Epoch: 351 \tTraining Loss: 33.292339 \tValidation Loss: 34.526722\n",
            "Epoch: 352 \tTraining Loss: 33.369614 \tValidation Loss: 35.202011\n",
            "Epoch: 353 \tTraining Loss: 35.001381 \tValidation Loss: 34.980732\n",
            "Epoch: 354 \tTraining Loss: 33.379601 \tValidation Loss: 34.701054\n",
            "Epoch: 355 \tTraining Loss: 33.296005 \tValidation Loss: 34.844101\n",
            "Epoch: 356 \tTraining Loss: 33.264847 \tValidation Loss: 35.410622\n",
            "Epoch: 357 \tTraining Loss: 33.444107 \tValidation Loss: 34.529175\n",
            "Epoch: 358 \tTraining Loss: 33.725079 \tValidation Loss: 34.676277\n",
            "Epoch: 359 \tTraining Loss: 33.364040 \tValidation Loss: 34.458534\n",
            "Epoch: 360 \tTraining Loss: 33.497055 \tValidation Loss: 34.668102\n",
            "Epoch: 361 \tTraining Loss: 33.293446 \tValidation Loss: 34.507492\n",
            "Epoch: 362 \tTraining Loss: 33.263100 \tValidation Loss: 35.271469\n",
            "Epoch: 363 \tTraining Loss: 33.250107 \tValidation Loss: 34.594254\n",
            "Epoch: 364 \tTraining Loss: 33.016872 \tValidation Loss: 34.482693\n",
            "Epoch: 365 \tTraining Loss: 33.027699 \tValidation Loss: 34.698582\n",
            "Epoch: 366 \tTraining Loss: 33.355259 \tValidation Loss: 34.261013\n",
            "Validation loss decreased (34.313354 --> 34.261013).  Saving model ...\n",
            "Epoch: 367 \tTraining Loss: 33.325844 \tValidation Loss: 35.327034\n",
            "Epoch: 368 \tTraining Loss: 33.121689 \tValidation Loss: 34.991959\n",
            "Epoch: 369 \tTraining Loss: 33.094040 \tValidation Loss: 34.685600\n",
            "Epoch: 370 \tTraining Loss: 37.212330 \tValidation Loss: 35.595409\n",
            "Epoch: 371 \tTraining Loss: 34.231068 \tValidation Loss: 35.524033\n",
            "Epoch: 372 \tTraining Loss: 33.726448 \tValidation Loss: 35.413303\n",
            "Epoch: 373 \tTraining Loss: 33.405361 \tValidation Loss: 34.869324\n",
            "Epoch: 374 \tTraining Loss: 33.722778 \tValidation Loss: 35.162422\n",
            "Epoch: 375 \tTraining Loss: 33.502220 \tValidation Loss: 34.851299\n",
            "Epoch: 376 \tTraining Loss: 33.216202 \tValidation Loss: 34.694233\n",
            "Epoch: 377 \tTraining Loss: 33.409054 \tValidation Loss: 34.721397\n",
            "Epoch: 378 \tTraining Loss: 33.480293 \tValidation Loss: 34.952251\n",
            "Epoch: 379 \tTraining Loss: 33.280407 \tValidation Loss: 34.792873\n",
            "Epoch: 380 \tTraining Loss: 33.602982 \tValidation Loss: 35.315380\n",
            "Epoch: 381 \tTraining Loss: 33.354843 \tValidation Loss: 34.879620\n",
            "Epoch: 382 \tTraining Loss: 33.557396 \tValidation Loss: 35.380009\n",
            "Epoch: 383 \tTraining Loss: 33.492401 \tValidation Loss: 35.149258\n",
            "Epoch: 384 \tTraining Loss: 33.382294 \tValidation Loss: 34.739998\n",
            "Epoch: 385 \tTraining Loss: 33.294056 \tValidation Loss: 34.821896\n",
            "Epoch: 386 \tTraining Loss: 33.351627 \tValidation Loss: 34.667728\n",
            "Epoch: 387 \tTraining Loss: 33.410965 \tValidation Loss: 35.244480\n",
            "Epoch: 388 \tTraining Loss: 33.399986 \tValidation Loss: 34.490479\n",
            "Epoch: 389 \tTraining Loss: 33.322178 \tValidation Loss: 35.077946\n",
            "Epoch: 390 \tTraining Loss: 33.238800 \tValidation Loss: 36.273979\n",
            "Epoch: 391 \tTraining Loss: 33.262699 \tValidation Loss: 34.931435\n",
            "Epoch: 392 \tTraining Loss: 32.991314 \tValidation Loss: 34.627647\n",
            "Epoch: 393 \tTraining Loss: 33.238445 \tValidation Loss: 35.114388\n",
            "Epoch: 394 \tTraining Loss: 33.380589 \tValidation Loss: 34.287003\n",
            "Epoch: 395 \tTraining Loss: 33.263176 \tValidation Loss: 34.982941\n",
            "Epoch: 396 \tTraining Loss: 33.224056 \tValidation Loss: 35.121254\n",
            "Epoch: 397 \tTraining Loss: 33.259716 \tValidation Loss: 34.806904\n",
            "Epoch: 398 \tTraining Loss: 33.308487 \tValidation Loss: 34.740414\n",
            "Epoch: 399 \tTraining Loss: 33.349442 \tValidation Loss: 35.487217\n",
            "Epoch: 400 \tTraining Loss: 33.399227 \tValidation Loss: 34.928120\n",
            "Epoch: 401 \tTraining Loss: 33.192875 \tValidation Loss: 34.856251\n",
            "Epoch: 402 \tTraining Loss: 33.243298 \tValidation Loss: 34.151577\n",
            "Validation loss decreased (34.261013 --> 34.151577).  Saving model ...\n",
            "Epoch: 403 \tTraining Loss: 33.132610 \tValidation Loss: 34.646816\n",
            "Epoch: 404 \tTraining Loss: 33.183052 \tValidation Loss: 34.484715\n",
            "Epoch: 405 \tTraining Loss: 33.094982 \tValidation Loss: 34.639000\n",
            "Epoch: 406 \tTraining Loss: 33.449112 \tValidation Loss: 35.663040\n",
            "Epoch: 407 \tTraining Loss: 33.241356 \tValidation Loss: 35.023159\n",
            "Epoch: 408 \tTraining Loss: 33.431351 \tValidation Loss: 35.154041\n",
            "Epoch: 409 \tTraining Loss: 33.096363 \tValidation Loss: 34.979744\n",
            "Epoch: 410 \tTraining Loss: 33.087444 \tValidation Loss: 34.636551\n",
            "Epoch: 411 \tTraining Loss: 33.206894 \tValidation Loss: 34.769138\n",
            "Epoch: 412 \tTraining Loss: 33.350407 \tValidation Loss: 34.732510\n",
            "Epoch: 413 \tTraining Loss: 33.223457 \tValidation Loss: 35.754276\n",
            "Epoch: 414 \tTraining Loss: 33.442562 \tValidation Loss: 35.394089\n",
            "Epoch: 415 \tTraining Loss: 37.554794 \tValidation Loss: 35.094917\n",
            "Epoch: 416 \tTraining Loss: 33.779751 \tValidation Loss: 34.815250\n",
            "Epoch: 417 \tTraining Loss: 33.522846 \tValidation Loss: 35.009010\n",
            "Epoch: 418 \tTraining Loss: 33.657974 \tValidation Loss: 34.426487\n",
            "Epoch: 419 \tTraining Loss: 33.636387 \tValidation Loss: 34.834557\n",
            "Epoch: 420 \tTraining Loss: 33.691078 \tValidation Loss: 35.275742\n",
            "Epoch: 421 \tTraining Loss: 33.498951 \tValidation Loss: 35.978756\n",
            "Epoch: 422 \tTraining Loss: 33.401226 \tValidation Loss: 35.116581\n",
            "Epoch: 423 \tTraining Loss: 33.529366 \tValidation Loss: 35.120785\n",
            "Epoch: 424 \tTraining Loss: 33.660770 \tValidation Loss: 34.958832\n",
            "Epoch: 425 \tTraining Loss: 33.695465 \tValidation Loss: 35.120815\n",
            "Epoch: 426 \tTraining Loss: 33.240425 \tValidation Loss: 34.410107\n",
            "Epoch: 427 \tTraining Loss: 48.748993 \tValidation Loss: 36.774174\n",
            "Epoch: 428 \tTraining Loss: 34.002930 \tValidation Loss: 34.926228\n",
            "Epoch: 429 \tTraining Loss: 33.626984 \tValidation Loss: 34.921177\n",
            "Epoch: 430 \tTraining Loss: 34.135246 \tValidation Loss: 36.402805\n",
            "Epoch: 431 \tTraining Loss: 33.820709 \tValidation Loss: 34.999775\n",
            "Epoch: 432 \tTraining Loss: 33.680340 \tValidation Loss: 34.566154\n",
            "Epoch: 433 \tTraining Loss: 33.829155 \tValidation Loss: 34.612782\n",
            "Epoch: 434 \tTraining Loss: 33.701893 \tValidation Loss: 34.753178\n",
            "Epoch: 435 \tTraining Loss: 33.249664 \tValidation Loss: 35.160870\n",
            "Epoch: 436 \tTraining Loss: 33.534767 \tValidation Loss: 35.552639\n",
            "Epoch: 437 \tTraining Loss: 33.487053 \tValidation Loss: 34.713116\n",
            "Epoch: 438 \tTraining Loss: 33.283737 \tValidation Loss: 34.538437\n",
            "Epoch: 439 \tTraining Loss: 33.336304 \tValidation Loss: 34.421986\n",
            "Epoch: 440 \tTraining Loss: 33.428623 \tValidation Loss: 34.571735\n",
            "Epoch: 441 \tTraining Loss: 33.313053 \tValidation Loss: 34.742645\n",
            "Epoch: 442 \tTraining Loss: 33.361351 \tValidation Loss: 34.634872\n",
            "Epoch: 443 \tTraining Loss: 33.300720 \tValidation Loss: 35.553452\n",
            "Epoch: 444 \tTraining Loss: 33.174141 \tValidation Loss: 34.708691\n",
            "Epoch: 445 \tTraining Loss: 33.183571 \tValidation Loss: 35.255157\n",
            "Epoch: 446 \tTraining Loss: 33.282875 \tValidation Loss: 34.882198\n",
            "Epoch: 447 \tTraining Loss: 33.419163 \tValidation Loss: 35.253258\n",
            "Epoch: 448 \tTraining Loss: 33.396286 \tValidation Loss: 35.321304\n",
            "Epoch: 449 \tTraining Loss: 33.603340 \tValidation Loss: 34.476761\n",
            "Epoch: 450 \tTraining Loss: 33.477962 \tValidation Loss: 34.810898\n",
            "Epoch: 451 \tTraining Loss: 33.146286 \tValidation Loss: 35.827675\n",
            "Epoch: 452 \tTraining Loss: 33.378162 \tValidation Loss: 34.883926\n",
            "Epoch: 453 \tTraining Loss: 33.338810 \tValidation Loss: 34.862888\n",
            "Epoch: 454 \tTraining Loss: 33.310467 \tValidation Loss: 35.127548\n",
            "Epoch: 455 \tTraining Loss: 33.150097 \tValidation Loss: 34.939472\n",
            "Epoch: 456 \tTraining Loss: 33.196178 \tValidation Loss: 34.730698\n",
            "Epoch: 457 \tTraining Loss: 32.928913 \tValidation Loss: 34.631271\n",
            "Epoch: 458 \tTraining Loss: 33.257015 \tValidation Loss: 35.092487\n",
            "Epoch: 459 \tTraining Loss: 33.413403 \tValidation Loss: 35.270679\n",
            "Epoch: 460 \tTraining Loss: 33.185822 \tValidation Loss: 35.471367\n",
            "Epoch: 461 \tTraining Loss: 33.168701 \tValidation Loss: 34.931507\n",
            "Epoch: 462 \tTraining Loss: 33.260830 \tValidation Loss: 35.905972\n",
            "Epoch: 463 \tTraining Loss: 33.498322 \tValidation Loss: 35.057098\n",
            "Epoch: 464 \tTraining Loss: 33.161209 \tValidation Loss: 34.663681\n",
            "Epoch: 465 \tTraining Loss: 33.316238 \tValidation Loss: 34.649254\n",
            "Epoch: 466 \tTraining Loss: 33.180073 \tValidation Loss: 34.312477\n",
            "Epoch: 467 \tTraining Loss: 33.134918 \tValidation Loss: 35.417625\n",
            "Epoch: 468 \tTraining Loss: 33.122730 \tValidation Loss: 34.311813\n",
            "Epoch: 469 \tTraining Loss: 33.147388 \tValidation Loss: 34.554131\n",
            "Epoch: 470 \tTraining Loss: 33.123714 \tValidation Loss: 34.908756\n",
            "Epoch: 471 \tTraining Loss: 33.248020 \tValidation Loss: 36.167721\n",
            "Epoch: 472 \tTraining Loss: 33.235016 \tValidation Loss: 35.261314\n",
            "Epoch: 473 \tTraining Loss: 33.084900 \tValidation Loss: 34.925465\n",
            "Epoch: 474 \tTraining Loss: 33.175995 \tValidation Loss: 34.775051\n",
            "Epoch: 475 \tTraining Loss: 33.343502 \tValidation Loss: 34.825184\n",
            "Epoch: 476 \tTraining Loss: 33.287819 \tValidation Loss: 34.977985\n",
            "Epoch: 477 \tTraining Loss: 33.638775 \tValidation Loss: 34.842571\n",
            "Epoch: 478 \tTraining Loss: 34.471230 \tValidation Loss: 34.739803\n",
            "Epoch: 479 \tTraining Loss: 33.641033 \tValidation Loss: 35.198822\n",
            "Epoch: 480 \tTraining Loss: 33.446789 \tValidation Loss: 35.434811\n",
            "Epoch: 481 \tTraining Loss: 33.302063 \tValidation Loss: 34.927048\n",
            "Epoch: 482 \tTraining Loss: 33.523220 \tValidation Loss: 34.902679\n",
            "Epoch: 483 \tTraining Loss: 33.171394 \tValidation Loss: 35.002495\n",
            "Epoch: 484 \tTraining Loss: 33.062336 \tValidation Loss: 35.560188\n",
            "Epoch: 485 \tTraining Loss: 33.087120 \tValidation Loss: 34.668568\n",
            "Epoch: 486 \tTraining Loss: 33.350700 \tValidation Loss: 35.168346\n",
            "Epoch: 487 \tTraining Loss: 33.013950 \tValidation Loss: 35.980770\n",
            "Epoch: 488 \tTraining Loss: 33.075603 \tValidation Loss: 35.853588\n",
            "Epoch: 489 \tTraining Loss: 33.102039 \tValidation Loss: 35.968620\n",
            "Epoch: 490 \tTraining Loss: 33.319263 \tValidation Loss: 34.799366\n",
            "Epoch: 491 \tTraining Loss: 33.253426 \tValidation Loss: 34.475533\n",
            "Epoch: 492 \tTraining Loss: 33.401524 \tValidation Loss: 35.020939\n",
            "Epoch: 493 \tTraining Loss: 33.283592 \tValidation Loss: 34.494209\n",
            "Epoch: 494 \tTraining Loss: 33.062363 \tValidation Loss: 34.825005\n",
            "Epoch: 495 \tTraining Loss: 33.091240 \tValidation Loss: 35.595413\n",
            "Epoch: 496 \tTraining Loss: 33.150940 \tValidation Loss: 36.235970\n",
            "Epoch: 497 \tTraining Loss: 33.322800 \tValidation Loss: 34.531895\n",
            "Epoch: 498 \tTraining Loss: 33.005104 \tValidation Loss: 35.011036\n",
            "Epoch: 499 \tTraining Loss: 33.195847 \tValidation Loss: 35.831142\n",
            "Epoch: 500 \tTraining Loss: 33.137234 \tValidation Loss: 35.392979\n",
            "Training Complete!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbepOlTjRFRP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "77a90250-b95e-4df2-f1b8-ea16025f8b46"
      },
      "source": [
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(valid_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5e9c2c0dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUVfr48c+T3hMSElqo0lQ6AUSkqit2VCzoKnx11XVdu6u4TdZdy/7WXctadrGya8GOrFhWEQVkRem9EzokgZDeJnN+f5w7mZlkQskkhonP+/XK6/Z7z52ZPPfcc849V4wxKKWUalnCmjsBSimlGp8Gd6WUaoE0uCulVAukwV0ppVogDe5KKdUCRTR3AgBat25tunTp0tzJUEqpkLJ06dI8Y0x6oGUnRHDv0qULS5Ysae5kKKVUSBGRHfUt02IZpZRqgTS4K6VUC6TBXSmlWiAN7kop1QJpcFdKqRZIg7tSSrVAGtyVUqoFCung/n32If76341UutzNnRSllDqhhHRwX7ojn79/uQWXW4O7UqHm4MGDDBgwgAEDBtC2bVs6dOhQM11ZWXnEbZcsWcLtt99+1GOcfvrpjZLWr776igsuuKBR9vVDOSGeUG0ocYb6vhGlQk9aWhorVqwAYNq0aSQkJHDvvffWLHe5XEREBA5RWVlZZGVlHfUYixYtapzEhqCQzrmHiQ3vGtuVahmmTJnCz3/+c4YNG8Z9993Hd999x/Dhwxk4cCCnn346GzduBPxz0tOmTeP6669nzJgxdOvWjaeffrpmfwkJCTXrjxkzhokTJ9K7d2+uueYaPG+h+/jjj+nduzeDBw/m9ttvP2oO/dChQ0yYMIF+/fpx2mmnsWrVKgC+/vrrmjuPgQMHUlRUxL59+xg1ahQDBgygT58+LFiwoNE/s/qEds7dybq7NeuuVFD+8J+1rNtb2Kj7PKV9Eg9eeOpxb7d7924WLVpEeHg4hYWFLFiwgIiICL744gt+/etf895779XZZsOGDcybN4+ioiJ69erFLbfcQmRkpN86y5cvZ+3atbRv354RI0bwzTffkJWVxc0338z8+fPp2rUrkyZNOmr6HnzwQQYOHMisWbP48ssvue6661ixYgWPP/44zz77LCNGjKC4uJiYmBimT5/OOeecw29+8xuqq6spLS097s+joUI6uHtobFeq5bj88ssJDw8HoKCggMmTJ7N582ZEhKqqqoDbnH/++URHRxMdHU1GRgYHDhwgMzPTb52hQ4fWzBswYADZ2dkkJCTQrVs3unbtCsCkSZOYPn36EdO3cOHCmgvMuHHjOHjwIIWFhYwYMYK7776ba665hksvvZTMzEyGDBnC9ddfT1VVFRMmTGDAgAFBfTbHI6SDu3iy7hrclQpKQ3LYTSU+Pr5m/He/+x1jx47lgw8+IDs7mzFjxgTcJjo6umY8PDwcl8vVoHWCMXXqVM4//3w+/vhjRowYwWeffcaoUaOYP38+c+bMYcqUKdx9991cd911jXrc+oR0mXtNhapGd6VapIKCAjp06ADAq6++2uj779WrF9u2bSM7OxuAt95666jbjBw5ktdffx2wZfmtW7cmKSmJrVu30rdvX+6//36GDBnChg0b2LFjB23atOHGG2/kZz/7GcuWLWv0c6hPSAf3ME/GXWO7Ui3SfffdxwMPPMDAgQMbPacNEBsby3PPPcf48eMZPHgwiYmJJCcnH3GbadOmsXTpUvr168fUqVOZMWMGAE8++SR9+vShX79+REZGcu655/LVV1/Rv39/Bg4cyFtvvcUdd9zR6OdQHzEnQGTMysoyDXlZx4xF2Tw4ey1Lf3sWaQnRR99AKaVqKS4uJiEhAWMMt956Kz169OCuu+5q7mQdExFZaowJ2CY0pHPuWuSulArWCy+8wIABAzj11FMpKCjg5ptvbu4kNYrQrlB1hifAzYdSKkTdddddIZNTPx4hnXOn5iEmje5KKeXrqMFdRF4WkRwRWeMzL1VEPheRzc6wlTNfRORpEdkiIqtEZFCTJt7bXEYppZSPY8m5vwqMrzVvKjDXGNMDmOtMA5wL9HD+bgKeb5xkBiZOwYxbg7tSSvk5anA3xswHDtWafTEwwxmfAUzwmf8vY30LpIhIu8ZKbG3eClWN7kop5auhZe5tjDH7nPH9QBtnvAOwy2e93c68OkTkJhFZIiJLcnNzG5QIrVBVKnSNHTuWzz77zG/ek08+yS233FLvNmPGjMHTbPq8887j8OHDddaZNm0ajz/++BGPPWvWLNatW1cz/fvf/54vvvjieJIf0InUNXDQFarGNpQ/7vBqjJlujMkyxmSlp6c36NjaFFKp0DVp0iRmzpzpN2/mzJnH1HkX2N4cU1JSGnTs2sH9oYce4qyzzmrQvk5UDQ3uBzzFLc4wx5m/B+jos16mM69JeMrcT4QHsZRSx2fixInMmTOn5sUc2dnZ7N27l5EjR3LLLbeQlZXFqaeeyoMPPhhw+y5dupCXlwfAww8/TM+ePTnjjDNqugUG24Z9yJAh9O/fn8suu4zS0lIWLVrE7Nmz+dWvfsWAAQPYunUrU6ZM4d133wVg7ty5DBw4kL59+3L99ddTUVFRc7wHH3yQQYMG0bdvXzZs2HDE82vuroEb2s59NjAZeMwZfugz/5ciMhMYBhT4FN80OtHuB5RqHJ9Mhf2rG3efbfvCuY/Vuzg1NZWhQ4fyySefcPHFFzNz5kyuuOIKRISHH36Y1NRUqqurOfPMM1m1ahX9+vULuJ+lS5cyc+ZMVqxYgcvlYtCgQQwePBiASy+9lBtvvBGA3/72t7z00kvcdtttXHTRRVxwwQVMnDjRb1/l5eVMmTKFuXPn0rNnT6677jqef/557rzzTgBat27NsmXLeO6553j88cd58cUX6z2/5u4a+FiaQr4J/A/oJSK7ReQGbFA/W0Q2A2c50wAfA9uALcALwC+CTuGR0wZocFcqVPkWzfgWybz99tsMGjSIgQMHsnbtWr8ilNoWLFjAJZdcQlxcHElJSVx00UU1y9asWcPIkSPp27cvr7/+OmvXrj1iejZu3EjXrl3p2bMnAJMnT2b+/Pk1yy+99FIABg8eXNPZWH0WLlzItddeCwTuGvjpp5/m8OHDREREMGTIEF555RWmTZvG6tWrSUxMPOK+j8VRc+7GmPoKwM4MsK4Bbg02UcdKe4VUqpEcIYfdlC6++GLuuusuli1bRmlpKYMHD2b79u08/vjjfP/997Rq1YopU6ZQXl7eoP1PmTKFWbNm0b9/f1599VW++uqroNLr6TY4mC6Df6iugUP6CVUtllEqtCUkJDB27Fiuv/76mlx7YWEh8fHxJCcnc+DAAT755JMj7mPUqFHMmjWLsrIyioqK+M9//lOzrKioiHbt2lFVVVXTTS9AYmIiRUVFdfbVq1cvsrOz2bJlCwD//ve/GT16dIPOrbm7Bg7tvmW0tYxSIW/SpElccsklNcUzni5ye/fuTceOHRkxYsQRtx80aBBXXnkl/fv3JyMjgyFDhtQs++Mf/8iwYcNIT09n2LBhNQH9qquu4sYbb+Tpp5+uqUgFiImJ4ZVXXuHyyy/H5XIxZMgQfv7znzfovDzvdu3Xrx9xcXF+XQPPmzePsLAwTj31VM4991xmzpzJX/7yFyIjI0lISOBf//pXg47pK6S7/P1wxR7umLmCL+8ZTbf0hCZImVJKnbhabJe/Htr9gFJK+Qvp4F7zDlUtmFFKKT+hHdyd4QlQsqSUUieU0A7uWqGqlFIBhXZwRx9iUkqpQEI6uIdpl79KKRVQSAd3T7GM29286VBKqRNNSAd3T5Wq5tyVUspfSAd37X5AKaUCC+3g3twJUEqpE1RIB/cw7fJXKaUCCungXlOhqtFdKaX8tIjgrqFdKaX8hXZw13eoKqVUQCEd3NGcu1JKBRRUcBeRO0RkjYisFZE7nXmpIvK5iGx2hq0aJ6kBju8MNeOulFL+GhzcRaQPcCMwFOgPXCAi3YGpwFxjTA9grjPdJLytZTS6K6WUr2By7icDi40xpcYYF/A1cClwMTDDWWcGMCG4JNZPK1SVUiqwYIL7GmCkiKSJSBxwHtARaGOM2eessx9oE2hjEblJRJaIyJLc3NwGJUB7hVRKqcAaHNyNMeuBPwP/BT4FVgDVtdYx1JOxNsZMN8ZkGWOy0tPTG5QGb/cDGt2VUspXUBWqxpiXjDGDjTGjgHxgE3BARNoBOMOc4JMZmL5kTymlAgu2tUyGM+yELW9/A5gNTHZWmQx8GMwxjnJ8QItllFKqtoggt39PRNKAKuBWY8xhEXkMeFtEbgB2AFcEm8j6aLGMUkoFFlRwN8aMDDDvIHBmMPs9Vloso5RSgYX0E6paLKOUUoGFeHC3Q30Tk1JK+Qvp4B6mb2JSSqmAQjq4e0rdtT93pZTyF9LBXbsfUEqpwEI7uHtGNLorpZSf0A7untYyGt2VUspPaAd3Z6hF7kop5S+kg3uYtnNXSqmAQjq4eypUtbWMUkr5C+ng7qGhXSml/IV0cBd9iEkppQIK7eCuXYcppVRAIR3cw5zUa85dKaX8hXRwl5ruB5o5IUopdYIJ7eCuvUIqpVRAoR3cnaEWyyillL/QDu7acZhSSgUU7Auy7xKRtSKyRkTeFJEYEekqIotFZIuIvCUiUY2V2AApAPQdqkopVVuDg7uIdABuB7KMMX2AcOAq4M/AE8aY7kA+cENjJDSQMDn6Okop9WMUbLFMBBArIhFAHLAPGAe86yyfAUwI8hj18vQKqd0PKKWUvwYHd2PMHuBxYCc2qBcAS4HDxhiXs9puoEOwiayPVqgqpVRgwRTLtAIuBroC7YF4YPxxbH+TiCwRkSW5ubkNTIMdanBXSil/wRTLnAVsN8bkGmOqgPeBEUCKU0wDkAnsCbSxMWa6MSbLGJOVnp7eoAR4HmLS2K6UUv6CCe47gdNEJE5s4feZwDpgHjDRWWcy8GFwSayfN+eu4V0ppXwFU+a+GFtxugxY7exrOnA/cLeIbAHSgJcaIZ0BabGMUkoFFnH0VepnjHkQeLDW7G3A0GD2e6z0HapKKRVYaD+h6gw1566UUv5CO7hr9wNKKRVQSAd3fUG2UkoFFtLB3VMso0+oKqWUv5AO7mixjFJKBRTSwb3mHaqac1dKKT+hHdw1566UUgGFdnB3hppxV0opfyEd3L2tZTS6K6WUr5AO7p5iGbfGdqWU8hPawV17hVRKqYBCOrijvUIqpVRAIR3cRd+hqpRSAYV0cNfuB5RSKrCQDu7a/YBSSgUW2sFdH2JSSqmAQju4o8UySikVSGgH95qcu0Z3pZTyFdLB3UNz7kop5a/BwV1EeonICp+/QhG5U0RSReRzEdnsDFs1ZoJ9hWlbSKWUCqjBwd0Ys9EYM8AYMwAYDJQCHwBTgbnGmB7AXGe6SdR0P6D9DyillJ/GKpY5E9hqjNkBXAzMcObPACY00jHqqOkVsqkOoJRSIaqxgvtVwJvOeBtjzD5nfD/QJtAGInKTiCwRkSW5ubkNOqjoQ0xKKRVQ0MFdRKKAi4B3ai8zttOXgKHXGDPdGJNljMlKT09v2LE9+9K8u1JK+WmMnPu5wDJjzAFn+oCItANwhjmNcIyARN+yp5RSATVGcJ+Et0gGYDYw2RmfDHzYCMcISPRlHUopFVBQwV1E4oGzgfd9Zj8GnC0im4GznOkmI6IVqkopVVtEMBsbY0qAtFrzDmJbz/wgBC2WUUqp2kL+CVUR0QpVpZSqJeSDe5hozl0ppWoL+eAuiL4gWymlagn54I5oO3ellKot5IO7gDaXUUqpWkI/uGtTSKWUqiP0gzuiDzEppVQtIR/ctbWMUkrVFfLBXURbyyilVG2hH9zR1jJKKVVbyAd3tFhGKaXqCPngrm9RVUqpukI+uIeFaWsZpZSqLeSDu4BWqCqlVC2hH9y1V0illKoj9IM7WqGqlFK1hX5w1+4HlFKqjpAP7iCac1dKqVqCfYdqioi8KyIbRGS9iAwXkVQR+VxENjvDVo2V2EDCtFtIpZSqI9ic+1PAp8aY3kB/YD0wFZhrjOkBzHWmm4wIuN1NeQSllAo9DQ7uIpIMjAJeAjDGVBpjDgMXAzOc1WYAE4JN5BHTgbaWUUqp2oLJuXcFcoFXRGS5iLwoIvFAG2PMPmed/UCbQBuLyE0iskREluTm5jY4EaLdDyilVB3BBPcIYBDwvDFmIFBCrSIYYx8dDRh6jTHTjTFZxpis9PT0BidC6juAUkr9iAUT3HcDu40xi53pd7HB/oCItANwhjnBJfHIRLS1jFJK1dbg4G6M2Q/sEpFezqwzgXXAbGCyM28y8GFQKTwKWyyj0V0ppXxFBLn9bcDrIhIFbAP+D3vBeFtEbgB2AFcEeYwj0oeYlFKqrqCCuzFmBZAVYNGZwez3eOg7VJVSqq6Qf0JVc+5KKVVXyAf3MH2HqlJK1RHywT08THBrdFdKKT8hH9wjw8OocGn/A0op5Svkg3tURBhV1RrclVLKV+gH93ChUnPuSinlJ/SDe0QYlZpzV0opP6Ef3MO1WEYppWoL+eAeGR6mxTJKKVVLyAf3qAgN7kopVVvLCO5aLKOUUn5CO7jvW8mow7Oorqpq7pQopdQJJbSD+7avmLD3CaiuaO6UKKXUCSW0g3uY7dTSXe1q5oQopdSJpYUEdy2WUUopXyEe3MMBMNXV2qe7Ukr5CPHgbnPu4VRrixmllPLRIoJ7BNVUVWvOXSmlPIJ6zZ6IZANFQDXgMsZkiUgq8BbQBcgGrjDG5AeXzHp4cu7itg8yRTfJUZRSKuQ0Rs59rDFmgDHG8y7VqcBcY0wPYK4z3TR8cu76lKpSSnk1RbHMxcAMZ3wGMKEJjmE5FarhuLXzMKWU8hFscDfAf0VkqYjc5MxrY4zZ54zvB9oE2lBEbhKRJSKyJDc3t2FH98m569uYlFLKK6gyd+AMY8weEckAPheRDb4LjTFGRALWdBpjpgPTAbKyshpWG+rbWkaDu1JK1Qgq526M2eMMc4APgKHAARFpB+AMc4JNZL1qcu5aLKOUUr4aHNxFJF5EEj3jwE+ANcBsYLKz2mTgw2ATWa+aMndt567Uj92jn6zn/KcXNHcyThjBFMu0AT4QEc9+3jDGfCoi3wNvi8gNwA7giuCTWQ+fnHt5VXWTHUYpdeL759fbmjsJJ5QGB3djzDagf4D5B4Ezg0nUMatp517NoZLKH+SQSikVClrME6o5hdrtr1JKeYR4cLdl7jHhhpyi8mZOjFJKnThCPLjbnHvr2DByizTnrpRSHi0iuLeKCSNHg7tSStXQ4K6UUi1QiwjuyTFCUXEJ7FsJG+aAvplJqR8tfXGPFeLB3VaoJkYJt1W9BP8cBTOvhgV/a+aEKaWai1tjOxB83zLNy8m5J0RCB9ninV+wq5kSpJRqbtVuQ3iYNHcyml2I59wjARvcXb7XKSdHr5T68anWrDsQ8sHdBvT4SHD5nopocFfqx8rl1n6mIOSDuw3i8ZHgMppzV0ppzt0jxIO7DehxEZpzV0pZLg3uQIsJ7gYXPgFdc+5K/Whpzt1qEcE9SgzpyfHe+RLap6WUajjNuVuhHQU9OXS3i4yUBO98oxUqSv1YVVdrcIdQb+cuTlvWrx8jOamrd3619u2u1I+VtpaxQjvn7iO6cLt3wqXd/yr1Y6Vl7laLCe5+XLVy7nMfgvdv9p9XsBv+1Bb2r/nh0qWUanLV2rcM0AjBXUTCRWS5iHzkTHcVkcUiskVE3hKRqOCTeZyqfXqIzN0EC/4Kq2b6r7PxE3CVwZKXfti0KaWalEvL3IHGybnfAaz3mf4z8IQxpjuQD9zQCMc4LpXlZc5ICTw7xLvA94peUxmrL9ZWqiXRYhkrqOAuIpnA+cCLzrQA44B3nVVmABOCOUZD5BwutCObPvNfUHrQOy4a3JVqibQppBVszv1J4D7AUz2dBhw2xric6d1AhyCPcdwOFxXbkR2L/BcU7fOOe3Luxie4PzP0+LoLztsCeZu902X5oDX1SjUrzblbDQ7uInIBkGOMWdrA7W8SkSUisiQ3N7ehyfDX/Syyk4fSs2Id7r9nwc7/+S8v9Anunhy7b849byPM/cOxH++ZwfBMlh0vOQh/7gILHm9Q0pVSjUObQlrB5NxHABeJSDYwE1sc8xSQIiKe9vOZwJ5AGxtjphtjsowxWenp6UEkw0d4FHFxcUSJi7CDm+FArZYwvv28Vznl8p6ce+0WNkfjW35f7YLvX7TjK944vv0opRqV5tytBgd3Y8wDxphMY0wX4CrgS2PMNcA8YKKz2mTgw6BTeSS9L/COh0eSGB9f/7q+xTRVpXboyblXFh/fcQt2e8fn/wW+esSON+Tp2GoXFO0//u2UUgC4fQK6lrlbTdHO/X7gbhHZgi2Db9q2hle9Dl1G2vHwKGJi4+pfd9s8qCiygbSiyM7zvG81UHCvKodHO8G3z/vPLz0Eaz/wTu9d5h0/1uCetxke6wSzfgGL/wF/7QU5G7zL138EH91VNz2VJce2f6V+RHwDunY/YDVKcDfGfGWMucAZ32aMGWqM6W6MudwYU3G07YMW5fQrEx6FhEcDsN205cyKv7C1zXi7rEOWbS3zaKYNpAuditONc2D7fPvnsfif9gGngt1QUQCfTvWvKJ37EHz+O+90iU+dgeeOAOxdwa7vbBHOgr/BN0/b+fk7bFl9eQGseB32LrfzF/3du+1b18CSl8Hl8/E9fzo80r4BH9BRbP4c1s5q/P0q9QOp1px7HaHdt4xHdKIdhkfW9BSZ0aY9W3d2YMXeEk4KB5IzYc+SwNvPuNB/+pP77HD/Su+83PXQ5lQ77lu8I2FwYK13uvQg7FkK2762TTF3fQtXvuatqG3VBeJb+x/P89DVls/thUB83v9YsBvSTrLjh7bW+xEct52LITwCOgy2xUo5G6DneIiMabxjKNXIvs8+RKfUONok+f9OfZ9KdesTqkBL6X4gqZ0dSjik9wYgXqp448ZhvFM9GoAVqece/353LvaOf/UoLHwCFj5pW9V4JHf0dlTWcZgdfvBzG8x3fWunt33lXf/ta2H/av/jHHYqeosPQM56W+xTs2yHHfr+YI+38jeQl38CL4yz44V77R3Kxo+D3299yguPvDxvM+RnN93xVZMoLK/i/WW7j75iI7n8H//j/KcX1pnvWxSjOXerZQT3Vl3ssPQgZDpPpB7axmld08hJHUKX8je48ovo49/vd/+0w5hkWP8f+GIafPGg/zqeXHV8Bpz9kB3P2+S/jqcljcfif/hP71sBPc6x4xvmwPrZ3mWHd9qh7wVh+b/8A3xFkfcCAbYIaceiY2tz73Z72/+vfLPu8oLdNmdfOzdkjL0z2bOs7ja1rX4XHusIB9bVv84zWfBUf+/09gUw517/49ZOQ+5GeOtaW7ylmsVvP1jD3W+vZM2epv8OPEUvecV1S3pdbjfJFJMpOVRrU0igpQT3lM52WLTPW3RyysWEhQmf3jmKjX8aT3SMT0Vrx9O845lD/feV3Knu/vteHvi4/a6CVCe4RydCSoBtEwOUkR/aVnde5hDoNNwG/q/+DOkn2zuRQ9tsUPvnSO+6c+6BP6XDtGRY+irMugWe7AOf/QbmPWqD9Cvnwn9/aythV7/rbRVUdMA/SBbuBrcL4tJgy1xvDtsYm44nToUv/2TTkbcFXrsMXjzb1ju8cQXMvCbwZ5OfbY9duM97sdq3wrt852J472e27x9fcx+C7IXw5iT4/gVvq6Qd/4M/pMC+VVDgtK5d+abd97xH/PdRkmfvnsoOey9wxTneyvPGVHLw+J5yXvGG/51ccysvsN9TA+0vsNsWljXBZ1tLcYWr3mXVxjAn+tcsjL6TiOL9UNxIz86EsJZR5u7JuRfuteXuv9pWUw4fFWGvX0O7pnHRhj9yiCRe65NGF0+RyU/fhdm3wTqnxWb7/lCw03//I+6wrVeKfZorTt0FkXE2AAHEJEFC27ppO+//QZczbOBMbGcD0vJ/e5dHJ9sikb4ToarE++DVpDfh0wfgm6dsBW99/nOHd/x/z9hht7F2uGomHFhtK4vDImwAn3EBXPiUdxtP0dOpl9g7jM9/DydfCK9d6n+cXYttTrrKaa2z+zs7LNpr72oS29nye+O2nbK9dY2t6PZthVScY3P78x7xBvrV78CFT3vXWfBX+5fQFiqL7HFSOsIiZ523fmpbO92zwXv3ku3cprur4eAW27pp5ZuwcqY959uXweM9oP/VcInT8qks39Yz7FgIQ2+yd2fHq6IY/tINTrsVxj9y9PXd1fZCDDCtwF54Vr4Jp1wM0QlH3tajqhzCoyDsKPmy4hxbvJfRO/Dy/9wJ+dvthabHOXDN28d2/FoiI2z9UIXrGHPLhXth+hi45l1o1++4jhUwuJcXQkwS1W5DpuQBcOHcM2GuwLTDx7X/47L6XYiMhd7nH3m9iiL7fUm4reMCe3e+7kO4dHrTpY+WknNPzrTD/lfZYXwaRPh3RvnIJX2YcP6FlMV1YMIH3uaEI59eyhOtflszbTKcnP/JF2EueIrSEVP5aGeEDSajp8JZf4D/+8QG8/AISO1m149OtP9wSbV6W0hsB7GtbPDuMsLbLj+pA6R1hztXwl3rILUrtPJ54Uj7AdDHCbCe/uknf2SHnjTWZ9s8Oyw96G0F9M5kG9gBvnvBu+7a9+3wlIvtcOkr/oF99FQ7nHWLN7B7eJqgvvVTePFMG1Q//70N7FC3eWnuRnj/Rv8cvOeYtXkupDuci52na+bDO2wFdN5m70NpuRttrvyjO+HZoT77M1CaB1u+sJMr37B3JB/eap8mfmW8vSv57+9sq6RD22H5a/aCaYwNvpU+rZ9q8xS/ffusd97rl8MHt9gc8dZ53vlut13ma+tc+PAX8Nxp9q7oaFwV8HAbmDvNO6+y1AaLylLvHdnqd+3F7Llh9mE9V6W9iBfneLdb+or3DmJzrT6YjkNkeBiRuDhcdoz1QBvm2Lql/z175PVclf53de5qIpdMJ4Ui77yNn9rivt1LA/QEeZzl7p//3vtb85W7yRYXHtruP/+9G2Dm1UfeZ0URPNEH/pThnwlb9HdY9RYsnu7fQq6RtYyce0Q0/DbX5trrkfTBUQYAABguSURBVJEUw/VndGVc7ww+Xbsf5oEbYdehMp6au5m7nMr3O78s46lwWJmdw207OlNU3p780uUsOT2f64bfRrf0WjmsmmKZJDu86WvY/rUtLsleAHGp/uv3Gg/3bIREn1x+bCtnX13918263hYz7V1ug07n0+H6/0K7/vaf/Ej6XAZr3gMEBlwDK17zLvN9cnfjx7aZaKfhMPJe/+4T2g+CM+6Erx/zzht5r/0HzV0P5/4Z3pniDXKfPeCfhk7Dbe5m65d2eqXz9O7lM6B1T3h+uJ32NAUNZO0HcPptde+mtnzuDe7uKvjLSTb4JWXaoiZfvh3I7V9tP8vay7d95a28Bvud5O+wF8pffOv/fa14034mI3z+YZe8bO9+Nv/XTm/8GMoPw8/m2t9G6UEbzD3c1d7PpWCXbeY6ZY7thvr7l2D8Y7bVVEIbmPew/Rw9v7FvnvLW73x4q/cCndwJrnnHBh6Ph9vaXKOptvv62Re2EUBtH98Hg66Dtn3stKfV1rJ/Qdt+NrMBtm4npaNdvm0ew8qW81L0H9n9zSjo+bK3JZjbbe9CC/fY321ypg12BfVUvn73gi1OG+v8huY9DN88Cbcts/VauxaTsfD3/CVyMDdW3W3X8dwBb/oUd/8AdygVRfbuZcaFcPGz0NXJjMy4CNr2hdH32ZZuYRH2M/3maf/c/up3vZ/lshk2o7PwbzD4/7zrFOdAQoYdz8+GJa/AqHvhtYk2c1Pu7M/TWKE4B3Y6pQaf/MoOe5wD6T0Dfy5BEHMCNBvKysoyS5bU00yxqRxYR5lE89+9MbywYBu/qvwHo4s+4prKB3g96lEWu3tzZeXv62zWOS2Oq4Z0Ii0hiksGdiDSuOw/UL8rqbjwGfYXlNM5LR6Kc3FvmMP8xPM4o3trIsLr3iS9t3Q3ndPiyOriXAAO77Jl52Bv249kmlOM8JOH4b+/obLv1Sw1PTlt29+R0oNw4zx4YSwMngLnPGJz1Rs/tncSG5w7gDG/hoOb4fy/2TsRsLmU/Gz7j37Oo7a44NGOUFEIZ9wNZz1oW/RkL4ShN9ptXr/cG9R8Pej8sBc9DbuXeMveH9ht73SmHaUoJDrJHhcBjA1e5QW2GMuj03D/PoR++p4NZv+upzNS3+Df/Ww4aVzdi1JtrbrYzzN3o707yHUeNkvqYIOXR89zYdMn/tu26VO3GwyArqPsXdVJ4+z38NJZgY89ZQ68GuDWv3Uv+93Vfmiubd+6rbE8wiJtYL7yde+FtbZzHoWMk6l690ZyBt9Nh4UPQEwKTN1hL+ozr7ZFKpGx8Or5lEss1W438eJUco5/zN5Bz7kX1rzrPe4dK2z9jUd0EnQbbS/y+Tu86976vQ1008d4L/qXvWQ/58/t/+NbrjFcOfmXts7GCZ4VaScTfdC353Hgxi9h0TP24pd1PVzwhA32/69WJsrXSWfCsJvh4Fb/30WPn0DPc2x910njvBfmsx+yxa6VJfD+Tbber/1Ab9oj4+CMu+zFavyf7Xksetr/mBc+Zf9PG0BElhpjsgIu+9EG93rsyckjbdbVLDv5PhaUZDJ/Uy5r9xbSPjmGvQX+FU/JsZGcc2obRm19nHWRfXi7dBB5xZWc2TuDxJgIDpdV8dVGW7Fzbp+2/HJcd95ftodu6fGM6ZXBiMfsD2TVtJ+QFBMJbjfmsY7kDbuf9DNvP3JCnx1mg8zUXTDnHv5UOYkXV5bx/uVpDJJNNjjvWwkZp9S9o9nyBcx/HH76PkTVeqJ30TOw6VOY8Jy3gvi50yFnLVzxbzjlojpJOVxayYfvvMIV3VzEnvEL+1BX8QEbaDxKD9mLwGm32CIqsDmrsAhbdr3pU9sqxzf3fNlLNkftyaHdvtz+E/3jDDud1sPmvlI6wcvn2OlbF9seP1fOtJXAX//ZrpvS2RbdFO317n/iy9B1NPy1ty1zL7Vltty1zuam966w+545yRuEfF8EA7biO6O3/xPLR3LNu/D6RO/0TV/ZYPDJVFj8vE3HgGvg2+f8txvza9vFhaeOJpCs6+0dhEdsqs09Vlfa8xz1K2/RnO86FYW2Ut0jPLrueQ7/pbdOJ6WTrfdwmgB/Wj2EHsluTip2+hCMSfHmWI9XVCKMvMtWrNdSGpfJiqIkTg/3aXU18h5bPJOzts76JLSxv8OwSFv3cuVrtkFAfZ9fbT1+EjjT4nGkC6lH19H27vffl3jnxWdAiVNE9vOF/v8nx0mDe5Bc1W5EhNJKF1+sP4AgJMdGMnvlXj5YHrBftKNKiomgsNz+Q53SLonRvdIpq6xm9sq9HCqp5PZx3UmOi+Lkdol0bBXHnNX7KCyr4sL+7Tm5XRKzFm9kx/487rh4BABX/ON/fJd9iLvP7snFA9qTlhBNQnQjlboV7LFt+7uO9naV7OPxzzbyzLwt/OqcXtw6tvtx7bqq2k2ly018dIS91S89BOs+sAEuIsYWZbx2Ga7oVkQ8kG032vQZRMXbHJNHcY4tSql9IZv1C5tLHP+oLRIoP2xzVbNvg1u/g/Re9m7AVWH3EZ3graD3ePcGm7NM7w2n/cLmlg+stZXpo++Hsb+2t+Mf3WkvpjnrbMAePdU+EGfc3iKkBw/b8mbjhoE/9S+2MwaqyiisjoAdi0ia6dSDdBkJUz6y9Q4ZJ9tgXFkC790IOxfB1W/b7yUuzeZ4wRYPth9g75hePNPmqE+7Bf6QaotoohJthfUZd8OQn9lc+fJ/2aBdksvn1YOJoYKRvTscsUx+eeQgnisZQ9LJY/nrqAhY/rqtxE/tBn0m2s933Sz7ne3+3m40aSZ8cr/3Qn7Oo7YeJnMIfHyvd+eXvWRzyS+eBdEJfN96Ajd+354nIp9jbPhKW2x40zwwhkNv/5LU9a/xjOtikoZdy7VV7yCr3oLOI+xF74ObvRewqAToMAiG32YvUG9dY4tG+l9li4C+edr+7q7/DOLT7W/j1fP9HyLsPwnOe9xe8Izh+11FbNi2nUlTXyBi33L7W3z/Jrj8FZux8C1GveBJ+1tJbA/31LrbOE4a3JvQ+n2FrN9XyBk9WlNWWU1keBj7Csq47Y3l3Dz6JM7t25adB0uZ+A9bdNC1dTwDO6WwYHMe95zdk1bxUdz7zkpKK6sxxhAZHnbUlgfpidHkFtmc1eDOrbj77J7c8tpSCstdtIqLJL+0inbJMQzrmkpkeBjXDu9M77ZJvL1kF19vyqXabZg4OJPk2Ejmb85ld34Z+SWVXD2sE/mlVXyzOY++mckkxUSQEhfF2N4ZHCgsp6raTUZiDAu35DG4cysSYyIIE+HW15fx9aZcRvdM58ELT+GTNftZvbuA/h1TuHpoJw6VVtK1deAO3X7x+lI+WbOfLQ+fR3iYbXlRWukiKjyMgyWVxEol2548j0dKJ9Bz2DlMPffkmotWQWkVf/9yM306JDNhYAcKy6vIK6ogMjyMhOgIUuIiEd+nfT08F5H4NPJLKomPjqhpVRWQ2237D2rbz1tRX5Zv735G3+dtabPpM+iQhamuZNIb2zi/fweuHd6Frzfm0DUqn07xpt7WK2v2FGAM9M1MZvyT89mwv4gt955MRGyybSBQX7oOrLZ1MJ7pT6fa+pCfvu+90B1YZy8KIvbJ6fWzbWAqzrHBy7flTUkexZvmc9pb1RQTx8rfn01y/mrM/jVIUnt7gdu1GD77NfSZyJlrf8LW3BK6pMXx5T1jCAsL8Hl7VDqtwbqfZb+DT+6zAfeCJ+zyahf80TnX62bbYhsfLy7Yxp/mrCcMNyvOWk/S0J9Ckm1qvDZ7H1++OJXnXRdRSgyndYrnoXGtaNPxZJLjo+xd2LoPbZGLb/2J222LbXqfb4ua6uN22wvEnqX2AjDxZe+T8UCXqXMAeO+W4QzunFp3+4I9tnWSMTZTkr/dfvY++2gIDe4ngOy8EkSgU2pcnYDjqnYTHiYUV7iodLlJiIlga04JrROjuOftleQWVXD9iK6c2iGJD5btYdaKvZRVuiip9LavToyJ4E8T+vDqomyW7zy+W+L4qHAqXO7jfrIvPEyOuXvVAR1TSIyJID0hmuyDJSzfdZhebRLZsN+2fhjTK53YyHCiI8KYvXIvvdomsX5fIbGR4ZRVec+zX2YyvdokEh8dwauLsmvm//q83sxZtY+Vu7233K3iIrkiqyMFZVWUVVWTX1qFMYaxvTJwud3syS/jtcU7Gd0znbvP7klRuYuoiDD6dEiitKIaA9z/3irO69uW/pkpbM0tQYCYyHAGd26Fy+1m5ne7+MmpbSipqOa1xTv4dutBrhzSkUc/seXyo3um8/WmXJJiInjk0r5UVbspdb63xJhIosLDGN4tjf4P2dv/9245ncuet91bXJGVyU9P68yhkkoyW8XRISWWLTnFxESG8cy8LaTGR3FR//Z0TI0jv6SSDq1iySmsYPmufMb0zKBVfN3XFxtjcLkN32zJY0iXVD5csZcN+wu548weJMZEYjB8u+0Qk1+2TV0/vHUE0xdsIzuvhAfOPZkzerT27AhEGPbIFxSWuWq+o2tP68zPRnZle14J8zbkkBwXRe+2iZx9ShvySyuJCAsjtZ50lVZWE1+UDTHJVMe1Zk9+GTGRYWQ4XQ08+cUmnvzCvhwnPiqc9imxDOmaSqu4SJ6dF7hrjpS4SK49rTOnn9SaU9rZeqXkOHvRq3YbVuzK5/1le5gwsAMDO6b41Y0dLq0kOTaSPYfLKCp3cbKzfW0FpVU139+VWR0Z1TOdEd3TSInzP89qt0HgyBfA46TBPYS53QYRAudAsXcOOw+VMqRLKqnxURhjqKo2LN2RT7mrmvbJsazYlc/0+du4qH8Hbj+zO+VVbv63LY/P1hxgWLdUxvXOICxMeGH+Ng6WVDKoUysGdEwhKTaCLTnFfL89n4hw4aWF2zlcWsmfJvQlr7iC4goXuUUVZLaK5RdjuvPRqr0UV7jo1SaRVXsKWL4zny835FBVbeiUGkdOUTnlVUdvD+1bZNW7bSJTTu9Ct/QEvt6Uw6zlezlcWllzYRvRPY1Kl5vvs/MBuHpYJ7q1jqfC5WbB5ly+3Wa7coiJDCM6IpwCn4dtEqMjKDrCgzFHEhMZdkznkpEYzZCuqcxZte+o63qIwJDOqXyXfejoK9cjOTaStkkxtE6MYt3eQjqnxTO0ayrvLt1NWWW13wUTbOagqPzon0X/zGSGdUvj++xD7DpURl5xBZOHd+brTblkH6y/2WhcVHjNRa1/xxRaxUWy42AplS433dLjWb+viLziCk5tn8TpJ6Xx8er97Dls37lw2aBMtuUVs3znYRKjI+ibmcz2vBJS46NYu9e/W4tbx57E7vwyxvXOYMHmPN5dWrd1TlxUOGN7Z7DlQDEbDxT5LWuXHMO43hlUuty8t2w3I7q3ZsFmWxfTMTWWq4d2JinWZr4OFJWTGB3BzO9tkdugTiksczJWQ7uk0qFVLPsKyrh+RFeemruZ9fsK6ZGRyM/HdOPN73YRHRHGGd1bc/Ypbeq2wjtGGtxVo3BVuzlUWklGYsM6FztUUkn2wRJOaZfEHTOXc0VWR9olx3JyO5uDr6p2s/dwGWf0SGfj/kL6ZaYQGaCVUU5hOY99soH+HVO4oF87IiPC+HztAXKL7R2Op4jF7TbsOFRK26QYwsIgytnXyt0FJMdG0rV1PNVuwwfL9+B2GzqmxnG4tJI3v99Fh5RY1uwp4Gcju7KvoJwPlu3hQFE5vz3/FKrdbr7ddojMVrHkFVeyePtBxvXKYHDnVvTpkMz8zbkUl7vI6pJKv8xkIsPDeOTj9azZU8DvLjiFhOgIDpZU8s2WPDqkxPLEF5vYcbCUv08ayIHCcoY42/3ls42UVLg4WFJJ26QYXlxo21pf0K8dw09Ko6TCxYZ9RewvtC209heUsfNQKb3bJfHx6n0IkJEYw/7CIz+Bev/43szbkMN32YfokBJL77aJDD8pjdiocL5cn8O4kzOIiQjnm6157DhYytId9kLqucD9+rzeXJHVkd35Zby3bDc5hRWkxEWSGh9Fv8wUwsPgszUH6J6RQEmli9kr97It1z4zMbhzK8oqqzm5XRKV1W7+t/VgwO4FUuLsxeq+8b0Y28s2PRQR1u0tpNptSE2I4nez1vDYpX1rcvoALy/cTl5xBRHhYZRXVeN2G/JLq/h49T7ap8RwzbDO9M1M5rY3ltM3M5nCsioWb/deVCPDhYToCPJL/Z/AFYH4qAiKK1z075jCFVmZXD20E+8s3c0/vt5ac34eUeFhDOuWyoqdh+tkKB67tC9XDQ3wdPsx0OCuVCNwVbsDNmkNljGGnKKKOj0d1lZQVkVuUQXdM46eyztUUkliTASR4WHsOlTKun2F9MhIsBeBwnLS4qMoLK9iSXY+5/W1He+53eaoRQbGGD5fd4AoJ9e5La+Erq3jA16Ej+Tj1fs4pV0SXQLUxRSVV3G4tIr2KbG4jWHt3kJ6tUkkNqpuZX5jqXabmjqf7LwS3vxuJ7eO605idAQigjEGEaGgrIrSShcxEeHERIZzsKSCzFZ13yGx6UARZZXV7C8sJyJMGNYtjYToCHKKytmTX0aXtHi+3pTLkK6pJMZE2NZyDaDBXSmlWqAjBfeW0f2AUkopPxrclVKqBdLgrpRSLVCDg7uIxIjIdyKyUkTWisgfnPldRWSxiGwRkbdEpG6jVqWUUk0qmJx7BTDOGNMfGACMF5HTgD8DTxhjugP5wA1H2IdSSqkm0ODgbixPh92Rzp8BxgFOF2/MAOrpnk8ppVRTCarMXUTCRWQFkAN8DmwFDhtjPK30dwMd6tn2JhFZIiJLcnP1lVhKKdWYggruxphqY8wAIBMYCtTzTq+A2043xmQZY7LS09ODSYZSSqlaGqVPWGPMYRGZBwwHUkQkwsm9ZwJH7RN36dKleSKy42jr1aM1kNfAbUOVnvOPg57zj0Mw59y5vgUNDu4ikg5UOYE9FjgbW5k6D5gIzAQmAx8ebV/GmAZn3UVkSX1PaLVUes4/DnrOPw5Ndc7B5NzbATNEJBxbvPO2MeYjEVkHzBSRPwHLgZcaIZ1KKaWOQ4ODuzFmFTAwwPxt2PJ3pZRSzaQlPKE6vbkT0Az0nH8c9Jx/HJrknE+IXiGVUko1rpaQc1dKKVWLBnellGqBQjq4i8h4EdnodFI2tbnT01hE5GURyRGRNT7zUkXkcxHZ7AxbOfNFRJ52PoNVIjKo+VLecCLSUUTmicg6pyO6O5z5Lfa8j7fzPRGJdqa3OMu7NGf6G8p5sn25iHzkTLfo8wUQkWwRWS0iK0RkiTOvSX/bIRvcnSaYzwLnAqcAk0TklOZNVaN5FRhfa95UYK4xpgcw15kGe/49nL+bgOd/oDQ2NhdwjzHmFOA04Fbn+2zJ5328ne/dAOQ7859w1gtFdwDrfaZb+vl6jDXGDPBp0960v21jTEj+YZ+G/cxn+gHggeZOVyOeXxdgjc/0RqCdM94O2OiM/xOYFGi9UP7DPvx29o/lvIE4YBkwDPu0YoQzv+Z3DnwGDHfGI5z1pLnTfpznmekEsnHAR4C05PP1Oe9soHWteU362w7ZnDu2Q7JdPtP1dlLWQrQxxuxzxvcDbZzxFvc5OLffA4HFtPDzPs7O92rO2VleAKT9sCkO2pPAfYDbmU6jZZ+vhwH+KyJLReQmZ16T/rYbpW8Z9cMyxhgRaZFtWEUkAXgPuNMYUygiNcta4nkbY6qBASKSAnzAcXS+F2pE5AIgxxizVETGNHd6fmBnGGP2iEgG8LmIbPBd2BS/7VDOue8BOvpMH1MnZSHsgIi0A3CGOc78FvM5iEgkNrC/box535nd4s8bbOd72H6Zajrfcxb5nlfNOTvLk4GDP3BSgzECuEhEsrF9T40DnqLlnm8NY8weZ5iDvYgPpYl/26Ec3L8Hejg17VHAVcDsZk5TU5qN7YgN/Dtkmw1c59SwnwYU+NzqhQyxWfSXgPXGmL/5LGqx5y0i6U6OHfF2vrceb+d7UPecPZ/FROBL4xTKhgJjzAPGmExjTBfs/+uXxphraKHn6yEi8SKS6BkHfgKsoal/281d0RBkJcV5wCZsOeVvmjs9jXhebwL7gCpsedsN2LLGucBm4Asg1VlXsK2GtgKrgazmTn8Dz/kMbLnkKmCF83deSz5voB+2c71Vzj/775353YDvgC3AO0C0Mz/Gmd7iLO/W3OcQxLmPAT76MZyvc34rnb+1nljV1L9t7X5AKaVaoFAullFKKVUPDe5KKdUCaXBXSqkWSIO7Ukq1QBrclVKqBdLgrpRSLZAGd6WUaoH+P3xPEDTCvMnTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pauKl5O78sG"
      },
      "source": [
        "def test(loaders, model, criterion, use_cuda):\n",
        "    \n",
        "    print(\"Begin Test!\")\n",
        "    \n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    \n",
        "    y_pred_list = []\n",
        "    target_list = []\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "          # move to GPU\n",
        "          if use_cuda:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(data)\n",
        "          output = output.view(-1)\n",
        "          target = target.unsqueeze(0).view(-1)\n",
        "          target_list.append(target.cpu().numpy())\n",
        "          # calculate the loss\n",
        "          loss = criterion(output, target)\n",
        "          # update average test loss \n",
        "          test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "          # convert output probabilities to predicted class\n",
        "          #plt.plot(torch.sigmoid(output.data))\n",
        "          pred = output.data\n",
        "          #pred = torch.round(torch.sigmoid(output.data))\n",
        "          y_pred_list.append(pred.cpu().numpy())\n",
        "          y_pred.append(output.data.cpu().numpy())\n",
        "          #pred = output.data.max(1, keepdim=True)[1]\n",
        "          # compare predictions to true label\n",
        "          correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "          total += data.size(0)\n",
        "\n",
        "    y_pred_list = [item for sublist in y_pred_list for item in sublist]\n",
        "    target_list = [item for sublist in target_list for item in sublist]\n",
        "\n",
        "    #fpr, tpr, thresholds = roc_curve(target_list, y_pred_list)\n",
        "    #roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "    \n",
        "    #print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (100. * correct / total, correct, total))\n",
        "\n",
        "    ypred = y_pred_list\n",
        "    Y_val = target_list\n",
        "\n",
        "    # Compute mean cross-validation score\n",
        "    #scores = cross_val_score(model, X_val, Y_val,cv=10)\n",
        "    #print(\"Mean cross-validation score: %.2f\" % scores.mean())\n",
        "    print(\"R2 score: %.2f\" % r2_score(Y_val, ypred))\n",
        "    print(\"Max error: %.2f\" % max_error(Y_val, ypred))\n",
        "    print(\"Explained Variance Score: %.2f\" % explained_variance_score(Y_val, ypred))\n",
        "    print('\\n')\n",
        "\n",
        "    # Compute Mean Squared Error (MSE)\n",
        "    mse = mean_squared_error(Y_val, ypred)\n",
        "    print(\"MSE: %.2f\" % mse)\n",
        "    print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
        "    print('\\n')\n",
        "\n",
        "    fig1 = plt.figure()\n",
        "    x_ax = range(len(Y_val))\n",
        "    plt.plot(x_ax, Y_val, label=\"original\")\n",
        "    plt.plot(x_ax, ypred, label=\"predicted\")\n",
        "    plt.title(\"Test and predicted data\")\n",
        "    plt.xlim(0, 100)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    #print('\\nConfusion Matrix:\\n')\n",
        "    #print(confusion_matrix(target_list, y_pred_list))\n",
        "\n",
        "    #print('\\nClassification Report:\\n')\n",
        "    #print(classification_report(target_list, y_pred_list))\n",
        "\n",
        "    # Calculate the ROC Curve and AUC score for the DNN Classifier\n",
        "    #print('\\nROC AUC Score:\\n')\n",
        "    #print(roc_auc_score(target_list, y_pred_list))\n",
        "\n",
        "    #plt.figure()\n",
        "    #lw = 2\n",
        "    #plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    #plt.xlabel('False Positive Rate')\n",
        "    #plt.ylabel('True Positive Rate')\n",
        "    #plt.title('Receiver operating characteristic')\n",
        "    #plt.legend(loc=\"lower right\")\n",
        "    #plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg8wbxkwDYfX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "29f184c0-c449-4ae4-810b-aad299b39038"
      },
      "source": [
        "# call test function    \n",
        "test(loaders, model, criterion, use_cuda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Test!\n",
            "Test Loss: 32.476784\n",
            "\n",
            "R2 score: 0.47\n",
            "Max error: 29.56\n",
            "Explained Variance Score: 0.47\n",
            "\n",
            "\n",
            "MSE: 32.47\n",
            "RMSE: 5.70\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe0-wjzBHmUw"
      },
      "source": [
        "def modelReg_eval(model, X_train, Y_train, X_test, Y_test, X_val, Y_val):\n",
        "  ''' Function to evaluate the performance of the regression model.'''\n",
        "\n",
        "  print(\"Model Evaluation:\\n\")\n",
        "  print(model)\n",
        "  print('\\n')\n",
        "  print(\"Accuracy score (training): {0:.3f}\".format(model.score(X_train, Y_train)))\n",
        "  print(\"Accuracy score (test): {0:.3f}\".format(model.score(X_test, Y_test)))\n",
        "  print(\"Accuracy score (validation): {0:.3f}\".format(model.score(X_val, Y_val)))\n",
        "  print('\\n')\n",
        "\n",
        "  y_pred = model.predict(X_val)\n",
        "  ypred = y_pred\n",
        "\n",
        "  # Compute mean cross-validation score\n",
        "  scores = cross_val_score(model, X_val, Y_val,cv=10)\n",
        "  print(\"Mean cross-validation score: %.2f\" % scores.mean())\n",
        "  print(\"R2 score: %.2f\" % r2_score(Y_val,y_pred))\n",
        "  print(\"Max error: %.2f\" % max_error(Y_val,y_pred))\n",
        "  print(\"Explained Variance Score: %.2f\" % explained_variance_score(Y_val,y_pred))\n",
        "  print('\\n')\n",
        "\n",
        "  # Compute Mean Squared Error (MSE)\n",
        "  mse = mean_squared_error(Y_val, ypred)\n",
        "  print(\"MSE: %.2f\" % mse)\n",
        "  print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
        "  print('\\n')\n",
        "\n",
        "  fig1 = plt.figure()\n",
        "  x_ax = range(len(Y_val))\n",
        "  plt.plot(x_ax, Y_val, label=\"original\")\n",
        "  plt.plot(x_ax, ypred, label=\"predicted\")\n",
        "  plt.title(\"Test and predicted data\")\n",
        "  plt.xlim(0, 100)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  print('\\n')\n",
        "\n",
        "  fig2 = plt.figure()\n",
        "  x_ax = range(len(Y_val))\n",
        "  plt.plot(x_ax, Y_val, label=\"original\")\n",
        "  plt.plot(x_ax, ypred, label=\"predicted\")\n",
        "  plt.title(\"Test and predicted data\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuOYg1JgJ3Xv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}